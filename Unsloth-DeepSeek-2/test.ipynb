{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneyi iÃ§e aktarma\n",
    "from unsloth import FastLanguageModel  # Unsloth kÃ¼tÃ¼phanesinden hÄ±zlÄ± model yÃ¼kleme sÄ±nÄ±fÄ±nÄ± iÃ§e aktarÄ±r, bÃ¼yÃ¼k dil modellerini optimize eder\n",
    "\n",
    "# PyTorch kÃ¼tÃ¼phanesini iÃ§e aktarma\n",
    "import torch                           # PyTorch'u tensÃ¶r iÅŸlemleri ve model hesaplamalarÄ± iÃ§in iÃ§e aktarÄ±r\n",
    "\n",
    "# Maksimum dizi uzunluÄŸu ayarÄ±\n",
    "max_seq_length = 2048                  # Modelin bir seferde iÅŸleyebileceÄŸi maksimum token sayÄ±sÄ±nÄ± 2048 olarak tanÄ±mlar, RoPE Scaling ile otomatik Ã¶lÃ§eklenir\n",
    "\n",
    "# Veri tipi ayarÄ±\n",
    "#dtype = None                           # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r; Tesla T4/V100 iÃ§in Float16, Ampere+ iÃ§in Bfloat16 seÃ§ilir\n",
    "dtype = torch.bfloat16 #A5000 ekran kartÄ± iÃ§in bunu seÃ§tik.\n",
    "\n",
    "# 4-bit kuantizasyon ayarÄ±\n",
    "load_in_4bit = True                    # Modeli 4-bit kuantizasyon ile yÃ¼kler, bu bellek kullanÄ±mÄ±nÄ± azaltÄ±r ve performansÄ± korur (False yapÄ±lÄ±rsa tam hassasiyet kullanÄ±lÄ±r)\n",
    "\n",
    "# 4-bit Ã¶nceden kuantize edilmiÅŸ modellerin listesi\n",
    "fourbit_models = [                     # Unslothâ€™un desteklediÄŸi, 4-bit kuantize edilmiÅŸ modellerin listesi; hÄ±zlÄ± indirme ve bellek tasarrufu saÄŸlar\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 8B modeli, 15 trilyon token ile eÄŸitilmiÅŸ, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  # Llama-3.1 8Bâ€™nin talimatlara Ã¶zel sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",     # Llama-3.1 70B modeli\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # Llama-3.1 405B modeli, 4-bit olarak yÃ¼kleniyor\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Mistral Nemo temel modeli, 12B, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",  # Mistral Nemoâ€™nun talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral 7B v0.3, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",   # Mistral 7B v0.3 talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 mini, talimatlara Ã¶zel, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",        # Phi-3 orta boy, 4k token kapasiteli\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",             # Gemma 2 9B modeli\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2 27B modeli, 2 kat hÄ±zlÄ±\n",
    "]  # Daha fazla model iÃ§in: https://huggingface.co/unsloth\n",
    "model_path = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "\n",
    "# Modeli ve tokenizer'Ä± Ã¶nceden eÄŸitilmiÅŸ olarak yÃ¼kleme\n",
    "model, tokenizer = FastLanguageModel.from_pretrained( \n",
    "   # force_download=True,\n",
    "      # Modeli ve tokenizer'Ä± yÃ¼kler, Unsloth optimizasyonlarÄ±yla\n",
    "    model_name=model_path,   # YÃ¼klenecek modelin adÄ±: LLaMA-3.1 8B (8 milyar parametre)\n",
    "    max_seq_length=max_seq_length,            # Maksimum dizi uzunluÄŸunu 2048 olarak ayarlar\n",
    "    dtype=dtype,                              # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r (Tesla T4 iÃ§in Float16 olur)\n",
    "    load_in_4bit=load_in_4bit,                # 4-bit kuantizasyonu etkinleÅŸtirir, belleÄŸi optimize eder\n",
    "    #token = \"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\",                       # EÄŸer kÄ±sÄ±tlÄ± eriÅŸimli bir model kullanÄ±yorsanÄ±z Hugging Face tokenâ€™Ä± eklenir (burada pasif)\n",
    ")  # Model ve tokenizer nesnelerini dÃ¶ndÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\tuners\\lora\\bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the LoRA adapters using PEFT\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\checkpoint-500\" # Path to the LoRA checkpoint\n",
    ")\n",
    "\n",
    "# Step 3: Merge LoRA into the base model and unload\n",
    "model = model.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\")\n",
    "tokenizer.save_pretrained(\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\")  # Tokenizerâ€™Ä± aynÄ± dizine kaydeder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim3\\\\dddd\",  # Path to the base model\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ± iÃ§in prompt ÅŸablonu tanÄ±mlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # ÃœÃ§ bÃ¶lÃ¼mden oluÅŸan bir ÅŸablon: talimat (instruction), giriÅŸ (input) ve cevap (response) yer tutucularÄ± iÃ§erir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "FenerbahÃ§e'nin son maÃ§larÄ±nÄ±n sonuÃ§larÄ±nÄ± listele ve her maÃ§ iÃ§in kÄ±sa bir aÃ§Ä±klama yap.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "        alpaca_prompt.format(           # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"FenerbahÃ§e'nin son maÃ§larÄ±nÄ±n sonuÃ§larÄ±nÄ± listele ve her maÃ§ iÃ§in kÄ±sa bir aÃ§Ä±klama yap.\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "            \"\",                         # GiriÅŸ: BoÅŸ bÄ±rakÄ±ldÄ±, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=512,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
