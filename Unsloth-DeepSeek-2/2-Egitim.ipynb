{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"GPU bulunamadÄ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"senin token\"  # Hugging Face Ã¼zerinden ince ayar iÃ§in kullanacaÄŸÄ±mÄ±z modeli indirmek iÃ§in Hugging Face tokenÄ± gerekmektedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneyi iÃ§e aktarma\n",
    "from unsloth import FastLanguageModel  # Unsloth kÃ¼tÃ¼phanesinden hÄ±zlÄ± model yÃ¼kleme sÄ±nÄ±fÄ±nÄ± iÃ§e aktarÄ±r, bÃ¼yÃ¼k dil modellerini optimize eder\n",
    "\n",
    "# PyTorch kÃ¼tÃ¼phanesini iÃ§e aktarma\n",
    "import torch                           # PyTorch'u tensÃ¶r iÅŸlemleri ve model hesaplamalarÄ± iÃ§in iÃ§e aktarÄ±r\n",
    "\n",
    "# Maksimum dizi uzunluÄŸu ayarÄ±\n",
    "max_seq_length = 2048                  # Modelin bir seferde iÅŸleyebileceÄŸi maksimum token sayÄ±sÄ±nÄ± 2048 olarak tanÄ±mlar, RoPE Scaling ile otomatik Ã¶lÃ§eklenir\n",
    "\n",
    "# Veri tipi ayarÄ±\n",
    "#dtype = None                           # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r; Tesla T4/V100 iÃ§in Float16, Ampere+ iÃ§in Bfloat16 seÃ§ilir\n",
    "dtype = torch.bfloat16 #A5000 ekran kartÄ± iÃ§in bunu seÃ§tik.\n",
    "\n",
    "# 4-bit kuantizasyon ayarÄ±\n",
    "load_in_4bit = True                    # Modeli 4-bit kuantizasyon ile yÃ¼kler, bu bellek kullanÄ±mÄ±nÄ± azaltÄ±r ve performansÄ± korur (False yapÄ±lÄ±rsa tam hassasiyet kullanÄ±lÄ±r)\n",
    "\n",
    "# 4-bit Ã¶nceden kuantize edilmiÅŸ modellerin listesi\n",
    "fourbit_models = [                     # Unslothâ€™un desteklediÄŸi, 4-bit kuantize edilmiÅŸ modellerin listesi; hÄ±zlÄ± indirme ve bellek tasarrufu saÄŸlar\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 8B modeli, 15 trilyon token ile eÄŸitilmiÅŸ, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  # Llama-3.1 8Bâ€™nin talimatlara Ã¶zel sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",     # Llama-3.1 70B modeli\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # Llama-3.1 405B modeli, 4-bit olarak yÃ¼kleniyor\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Mistral Nemo temel modeli, 12B, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",  # Mistral Nemoâ€™nun talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral 7B v0.3, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",   # Mistral 7B v0.3 talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 mini, talimatlara Ã¶zel, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",        # Phi-3 orta boy, 4k token kapasiteli\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",             # Gemma 2 9B modeli\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2 27B modeli, 2 kat hÄ±zlÄ±\n",
    "]  # Daha fazla model iÃ§in: https://huggingface.co/unsloth\n",
    "model_path = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim\\\\merged_model\"\n",
    "\n",
    "# Modeli ve tokenizer'Ä± Ã¶nceden eÄŸitilmiÅŸ olarak yÃ¼kleme\n",
    "model, tokenizer = FastLanguageModel.from_pretrained( \n",
    "   # force_download=True,\n",
    "      # Modeli ve tokenizer'Ä± yÃ¼kler, Unsloth optimizasyonlarÄ±yla\n",
    "    model_name=model_path,   # YÃ¼klenecek modelin adÄ±: LLaMA-3.1 8B (8 milyar parametre)\n",
    "    max_seq_length=max_seq_length,            # Maksimum dizi uzunluÄŸunu 2048 olarak ayarlar\n",
    "    dtype=dtype,                              # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r (Tesla T4 iÃ§in Float16 olur)\n",
    "    load_in_4bit=load_in_4bit,                # 4-bit kuantizasyonu etkinleÅŸtirir, belleÄŸi optimize eder\n",
    "    #token = \"senin token\",                       # EÄŸer kÄ±sÄ±tlÄ± eriÅŸimli bir model kullanÄ±yorsanÄ±z Hugging Face tokenâ€™Ä± eklenir (burada pasif)\n",
    ")  # Model ve tokenizer nesnelerini dÃ¶ndÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR =  \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim2\" #Model verilerini kayÄ±t edeceÄŸiniz klasÃ¶rÃ¼ seÃ§in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ± iÃ§in prompt ÅŸablonu tanÄ±mlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # ÃœÃ§ bÃ¶lÃ¼mden oluÅŸan bir ÅŸablon: talimat (instruction), giriÅŸ (input) ve cevap (response) yer tutucularÄ± iÃ§erir\n",
    "\n",
    "# EOS_TOKEN tanÄ±mlama\n",
    "EOS_TOKEN = tokenizer.eos_token  # Tokenizer'dan EOS (End of Sequence) token'Ä±nÄ± alÄ±r, metnin sonunu iÅŸaretler\n",
    "\n",
    "# Veri setini biÃ§imlendirme fonksiyonu\n",
    "def formatting_prompts_func(examples):  # Veri setindeki Ã¶rnekleri Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in bir fonksiyon\n",
    "    instructions = examples[\"instruction\"]  # Veri setindeki \"instruction\" sÃ¼tununu alÄ±r (talimatlar)\n",
    "    inputs       = examples[\"input\"]        # Veri setindeki \"input\" sÃ¼tununu alÄ±r (giriÅŸler)\n",
    "    outputs      = examples[\"output\"]       # Veri setindeki \"output\" sÃ¼tununu alÄ±r (cevaplar)\n",
    "    texts = []                              # BiÃ§imlendirilmiÅŸ metinleri saklamak iÃ§in boÅŸ bir liste oluÅŸturur\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):  # Her bir talimat, giriÅŸ ve cevap Ã¼Ã§lÃ¼sÃ¼nÃ¼ eÅŸleÅŸtirir\n",
    "        # Alpaca ÅŸablonunu doldurur ve EOS_TOKEN ekler, yoksa Ã¼retim sonsuza dek devam eder!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN  # Åžablonu doldurur ve metnin sonuna EOS tokenâ€™Ä± ekler\n",
    "        texts.append(text)                  # BiÃ§imlendirilmiÅŸ metni listeye ekler\n",
    "    return { \"text\" : texts, }              # \"text\" anahtarÄ±yla biÃ§imlendirilmiÅŸ metinleri bir sÃ¶zlÃ¼k olarak dÃ¶ndÃ¼rÃ¼r\n",
    "pass                                       # BoÅŸ bir \"pass\" ifadesi (gereksiz, fonksiyon zaten tamamlandÄ±)\n",
    "\n",
    "# Veri setini yÃ¼kleme ve iÅŸleme\n",
    "from datasets import load_dataset       # Hugging Faceâ€™in datasets kÃ¼tÃ¼phanesini iÃ§e aktarÄ±r\n",
    "dataset = load_dataset(\"json\", data_files=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\alpaca_format.json\")[\"train\"]  # JSON dosyasÄ±ndan veri setini yÃ¼kler ve \"train\" bÃ¶lÃ¼mÃ¼nÃ¼ alÄ±r\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)  # Veri setine biÃ§imlendirme fonksiyonunu toplu (batched) ÅŸekilde uygular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daha sonra tekrar kullanmak iÃ§in:\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\model_egitim\\\\final_model\\\\tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 10351, 23684, 2322, 7907, 104250, 4355, 382, 14711, 5688, 512, 605, 13, 69657, 88, 2448, 18126, 120764, 24277, 809, 117545, 198, 68786, 5320, 123615, 9736, 24082, 1543, 58183, 108780, 113153, 5320, 14634, 352, 3458, 4265, 14966, 113086, 58804, 2448, 48497, 2448, 198, 68786, 5320, 123615, 9736, 24082, 1543, 58183, 108780, 113153, 5320, 14634, 352, 3458, 4265, 14966, 113086, 58804, 2448, 48497, 2448, 17773, 100805, 55809, 1008, 9655, 277, 80726, 480, 483, 4265, 68, 40528, 111508, 58112, 101766, 767, 1270, 112353, 104383, 24277, 587, 462, 69657, 88, 2448, 18126, 3209, 13532, 110560, 1543, 11, 330, 98581, 318, 127113, 112839, 119666, 64010, 101492, 26, 120781, 513, 10176, 88, 15686, 103611, 94582, 45356, 295, 74853, 13, 480, 30808, 5888, 305, 8115, 982, 1048, 10167, 318, 101149, 258, 102219, 55598, 8093, 5320, 108189, 107260, 899, 261, 6729, 409, 58804, 125600, 1210, 103095, 627, 28703, 74709, 333, 122550, 69, 3862, 126564, 3567, 1171, 23684, 5509, 620, 277, 114088, 110807, 1270, 112353, 104383, 3765, 307, 23685, 69657, 88, 2448, 18126, 3209, 13532, 9419, 969, 119371, 11, 330, 42, 26844, 101393, 13835, 3067, 85067, 333, 122550, 69, 3862, 126564, 3567, 1171, 23684, 1077, 111734, 320, 37, 1372, 64461, 113994, 107902, 696, 14711, 6075, 512, 11, 127113, 101934, 4282, 103683, 103009, 105013, 109323, 114648, 106760, 1029, 277, 16507, 520, 104050, 34271, 114088, 110807, 1210, 103095, 627, 17773, 12341, 25041, 4265, 64, 101054, 59385, 668, 1494, 23306, 1565, 9020, 258, 81438, 111162, 1270, 112353, 104383, 15367, 31303, 301, 309, 69657, 88, 2448, 18126, 3209, 13532, 14925, 5460, 1564, 11257, 10036, 11, 58780, 25041, 31905, 102003, 107321, 112637, 342, 12574, 99439, 103279, 112461, 101813, 126914, 96548, 110387, 102733, 37947, 9988, 28726, 11, 102003, 104383, 59385, 668, 1494, 23306, 7792, 119730, 665, 104268, 9637, 1494, 71, 29758, 116126, 125049, 58780, 25041, 102733, 110143, 627, 47055, 450, 85794, 4265, 68, 106395, 23119, 9603, 273, 78381, 10448, 102851, 8492, 20920, 41990, 1270, 112353, 104383, 61021, 2836, 7370, 84, 69657, 88, 2448, 18126, 3209, 13532, 507, 12082, 276, 71319, 277, 11, 330, 98581, 11, 85794, 4265, 68, 106395, 23119, 9603, 273, 78381, 10448, 102851, 8492, 20920, 41990, 13, 114081, 15606, 117045, 117818, 106395, 84240, 110140, 1210, 103095, 13, 128001, 128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 10351, 23684, 2322, 7907, 104250, 4355, 382, 14711, 5688, 512, 868, 111433, 109551, 12558, 6344, 258, 1676, 3306, 276, 105058, 98492, 102614, 100965, 104784, 71862, 342, 324, 1103, 84, 198, 37, 1372, 64461, 44886, 16461, 15627, 1395, 342, 21336, 7370, 25877, 111162, 104014, 109459, 118327, 124814, 117610, 109551, 275, 57398, 6344, 258, 1676, 3306, 276, 105058, 98492, 102614, 57331, 4919, 1676, 3306, 276, 5320, 26417, 1764, 2468, 13005, 11, 113499, 102645, 757, 57000, 38511, 104223, 104223, 112301, 113733, 121778, 101488, 117683, 1169, 103422, 78587, 100961, 627, 77489, 370, 40957, 198, 37, 295, 41897, 106610, 10335, 9603, 35137, 2026, 29758, 2448, 44886, 16461, 320, 37, 1372, 64461, 8, 15627, 1395, 342, 21336, 7370, 25877, 111162, 104014, 109459, 118327, 124814, 117610, 109551, 275, 57398, 6344, 258, 1676, 3306, 276, 105058, 13528, 370, 40957, 956, 68, 118743, 102550, 671, 114973, 11, 121778, 101488, 15606, 81967, 3059, 117683, 1169, 74853, 34271, 104223, 104223, 113499, 102645, 75095, 81, 6729, 112301, 108111, 627, 42, 42441, 7370, 102645, 111819, 273, 9978, 72, 2206, 57000, 102320, 6, 303, 14966, 75095, 81, 6729, 112301, 104225, 26417, 1764, 2468, 13005, 11, 29334, 12097, 10796, 404, 483, 11, 15627, 1395, 342, 21336, 7370, 318, 6729, 3721, 103994, 110228, 450, 88, 2159, 276, 102046, 9484, 1609, 101611, 11, 113499, 65544, 127913, 1050, 32752, 58628, 88, 575, 115026, 105058, 109685, 81, 90513, 104997, 120392, 101017, 28985, 15627, 17106, 108495, 101901, 10878, 118668, 110143, 627, 46, 6789, 113734, 102858, 105527, 276, 2468, 13005, 11, 330, 98581, 409, 297, 88787, 122058, 4815, 14711, 6075, 512, 1964, 12841, 68945, 110228, 450, 88, 2159, 276, 102046, 9484, 1609, 11, 22924, 125279, 13, 124541, 13338, 67793, 101165, 18551, 360, 118668, 31285, 6634, 40957, 13, 6344, 258, 31905, 104265, 48497, 3178, 11, 43250, 4355, 48497, 41990, 114395, 114299, 220, 717, 4265, 268, 85226, 59382, 453, 309, 114102, 13, 109865, 85226, 113499, 61334, 348, 830, 266, 103040, 101588, 102046, 9484, 1609, 1210, 103132, 118993, 627, 1688, 13005, 11, 24318, 451, 79995, 982, 342, 324, 324, 102082, 1072, 10784, 277, 38511, 348, 5673, 360, 352, 28985, 1359, 42, 42441, 7370, 318, 273, 103611, 43478, 264, 18280, 72, 37947, 92057, 1609, 13, 37919, 125111, 85226, 459, 3520, 72, 11, 220, 18, 125111, 85226, 16993, 100849, 37947, 92057, 1609, 13, 220, 23, 100901, 58100, 13, 13528, 109911, 110945, 113499, 101256, 11, 86997, 110945, 3008, 111129, 11, 293, 12273, 110945, 16993, 117818, 71862, 109551, 275, 11, 102003, 104383, 109551, 12558, 13, 115433, 342, 324, 324, 2807, 3457, 15606, 123150, 13, 13528, 370, 40957, 956, 268, 220, 868, 111433, 4265, 64, 102691, 798, 72, 115589, 79765, 11, 112455, 258, 6348, 3536, 6729, 79705, 3197, 64, 127631, 79765, 15606, 113499, 124711, 5964, 75437, 34271, 58112, 342, 324, 1103, 4168, 5308, 13, 31612, 89, 2448, 509, 40957, 71862, 342, 324, 1103, 4168, 5308, 1210, 113374, 109992, 597, 620, 438, 3862, 627, 45, 324, 4919, 1676, 3306, 276, 3067, 113499, 6729, 37947, 92057, 115763, 34271, 100965, 104784, 11, 102114, 103577, 110929, 2453, 118581, 107733, 112667, 342, 324, 1103, 84, 105215, 101488, 5320, 3686, 86626, 103485, 409, 72848, 110744, 109148, 105307, 20920, 93687, 38511, 37947, 67, 29037, 13, 128001, 128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 10351, 23684, 2322, 7907, 104250, 4355, 382, 14711, 5688, 512, 868, 111433, 79919, 2701, 122157, 40424, 118039, 5320, 473, 2642, 112870, 220, 868, 111433, 64730, 32893, 72, 6, 818, 112301, 1565, 101235, 102980, 198, 37, 1372, 64461, 44886, 16461, 220, 23, 100901, 118172, 305, 467, 15627, 1395, 342, 21336, 7370, 318, 6729, 110088, 107706, 34271, 120338, 110470, 110929, 125036, 96548, 104511, 82553, 3862, 31905, 102691, 798, 68, 602, 10118, 295, 104225, 15627, 17106, 103485, 597, 620, 438, 75437, 296, 22284, 318, 8637, 16507, 1446, 61887, 41124, 44907, 330, 39, 2642, 112870, 220, 868, 111433, 64730, 32893, 72, 1, 5320, 330, 868, 111433, 79919, 2701, 122157, 40424, 118039, 1, 3458, 112301, 1565, 103412, 108111, 627, 120447, 198, 868, 111433, 79919, 2701, 122157, 65994, 67793, 82, 2448, 44886, 16461, 113467, 71968, 93385, 342, 21336, 7370, 103683, 220, 868, 111433, 79919, 2701, 122157, 40424, 118039, 5320, 473, 2642, 112870, 220, 868, 111433, 64730, 32893, 72, 6, 818, 11, 330, 868, 111433, 4829, 564, 81, 10426, 5320, 123316, 37919, 7792, 480, 103314, 1, 107733, 112667, 120897, 16507, 122938, 52465, 104913, 113490, 120699, 112301, 1565, 122058, 3067, 101235, 102234, 627, 53, 266, 112996, 14115, 11, 102851, 220, 12326, 109551, 307, 258, 1008, 80726, 55295, 116126, 220, 868, 111433, 79919, 2701, 122157, 40424, 118039, 44886, 3862, 112301, 108111, 13, 120931, 19091, 277, 118929, 276, 112301, 120764, 11, 1077, 127174, 107664, 15606, 109551, 307, 258, 374, 8318, 5320, 101213, 114705, 6729, 118914, 597, 548, 74, 77055, 104623, 55505, 124440, 102154, 122064, 28074, 294, 8083, 78413, 48320, 627, 64461, 668, 117680, 1048, 106251, 12874, 103810, 115122, 14115, 11, 15627, 1395, 342, 21336, 7370, 318, 6729, 110088, 107706, 34271, 120338, 110470, 102191, 107902, 104511, 7911, 31905, 15627, 17106, 103485, 597, 620, 438, 75437, 296, 22284, 318, 8637, 16507, 1446, 61887, 41124, 44907, 473, 2642, 112870, 220, 868, 111433, 64730, 32893, 72, 6, 7907, 409, 112301, 108111, 13, 55347, 23684, 268, 104511, 88, 100849, 125496, 107858, 112301, 120764, 11, 9072, 3862, 969, 26656, 11257, 15817, 3862, 3067, 102565, 84442, 627, 68149, 1494, 26248, 58112, 22841, 40957, 5297, 105013, 336, 34335, 597, 5757, 569, 41990, 702, 868, 111433, 79919, 2701, 122157, 40424, 118039, 44886, 3862, 112301, 104225, 104550, 23404, 1065, 301, 105580, 276, 11, 305, 467, 15627, 1395, 342, 21336, 7370, 61334, 258, 84668, 438, 75437, 114299, 220, 868, 111433, 79919, 2701, 122157, 65994, 67793, 82, 2448, 6, 818, 108432, 109824, 106753, 101588, 110143, 627, 53773, 17106, 72, 113408, 102191, 101901, 10878, 1072, 10784, 277, 38511, 105379, 109551, 275, 7792, 52016, 309, 80726, 72848, 111303, 17580, 575, 117426, 105527, 276, 105580, 276, 11, 330, 62339, 2649, 109551, 275, 57398, 4355, 271, 14711, 6075, 512, 81, 16507, 568, 1725, 65544, 103056, 46937, 303, 276, 17372, 79, 119817, 13, 28471, 342, 5676, 342, 5676, 436, 1494, 4150, 384, 2552, 258, 13, 28471, 11, 118924, 8826, 101492, 36255, 3067, 24788, 3862, 110642, 13, 123520, 18728, 5136, 101246, 113789, 274, 31866, 112306, 113044, 1210, 103095, 627, 5519, 124088, 11, 15627, 1395, 342, 21336, 7370, 25877, 113734, 101770, 1048, 101934, 3640, 68, 84668, 329, 91166, 68228, 61822, 102577, 11, 330, 101579, 324, 2649, 102550, 34670, 108497, 13, 1666, 7197, 658, 65, 5014, 6729, 13845, 78075, 7370, 22841, 486, 3933, 450, 1565, 108497, 13, 105664, 119993, 101901, 123517, 113408, 7792, 109452, 43478, 1565, 348, 569, 41990, 13, 507, 22841, 486, 3933, 450, 47879, 110608, 379, 408, 1609, 13, 23404, 28471, 26248, 58112, 22841, 40957, 5297, 105013, 336, 34335, 597, 5757, 569, 41990, 13, 469, 2727, 44907, 110278, 1077, 441, 16172, 28471, 24788, 3862, 110642, 1210, 103132, 118993, 627, 33450, 359, 8733, 11, 109551, 275, 20717, 2008, 352, 35137, 1195, 20442, 21291, 336, 404, 52152, 374, 37511, 108874, 101588, 101664, 2002, 105580, 276, 11, 330, 21364, 12841, 258, 1048, 106060, 120490, 374, 37511, 5801, 359, 102552, 15593, 1631, 93385, 125624, 13, 28471, 104552, 109551, 275, 109794, 5320, 389, 36255, 24788, 3862, 110642, 1210, 113374, 109992, 597, 620, 438, 3862, 627, 35, 727, 64, 24610, 3862, 564, 3067, 305, 467, 15627, 1395, 342, 21336, 7370, 61334, 258, 84668, 438, 75437, 114299, 220, 868, 111433, 79919, 2701, 122157, 65994, 67793, 82, 2448, 6, 818, 43019, 1609, 101611, 105379, 342, 556, 2106, 1609, 101611, 37947, 9988, 28726, 11, 330, 105812, 101235, 2448, 109551, 275, 109794, 58100, 13, 28471, 124853, 101488, 272, 2734, 295, 220, 1441, 258, 11, 120985, 597, 5757, 569, 3862, 14115, 13, 10641, 5641, 14115, 118077, 11, 98602, 346, 11257, 12117, 11, 104927, 14115, 118077, 13728, 101488, 26742, 5888, 13, 28326, 348, 16623, 110945, 3826, 13, 1666, 4355, 56884, 8674, 68, 96998, 1216, 450, 13, 119005, 14115, 118077, 106527, 84668, 2629, 348, 16623, 513, 7387, 106290, 102349, 111853, 5308, 1210, 103095, 627, 53773, 1395, 342, 21336, 7370, 61334, 258, 84668, 438, 75437, 114299, 39172, 84668, 80726, 106527, 104337, 109905, 9072, 3862, 2438, 109605, 2629, 103243, 1974, 93687, 38511, 108524, 104225, 473, 49621, 1216, 969, 30011, 77193, 127913, 1050, 32752, 58628, 88, 575, 115026, 105058, 109685, 81, 112667, 107260, 36255, 101352, 111876, 115122, 101352, 120980, 103040, 627, 2127, 112288, 41757, 117545, 3566, 110803, 11, 29334, 29976, 261, 16762, 33573, 114901, 22530, 320, 79330, 8, 108107, 671, 263, 109417, 7160, 51835, 59382, 1565, 11, 17372, 61828, 2963, 28726, 98659, 3862, 1029, 276, 101891, 13, 3765, 263, 102182, 34271, 326, 2448, 9112, 268, 31905, 10462, 7370, 547, 77211, 70739, 13, 128001, 128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 10351, 23684, 2322, 7907, 104250, 4355, 382, 14711, 5688, 512, 868, 111433, 109551, 275, 47879, 13665, 450, 127125, 101165, 16993, 106373, 1645, 90513, 11884, 120449, 198, 37, 295, 41897, 106610, 10335, 9603, 35137, 2026, 29758, 2448, 44886, 16461, 320, 37, 1372, 64461, 8, 220, 868, 111433, 220, 679, 21, 4265, 14966, 15627, 1395, 342, 21336, 7370, 25877, 111162, 71344, 480, 50902, 4749, 78413, 6, 303, 14966, 13054, 100923, 111474, 118250, 109551, 275, 57398, 127125, 1871, 21585, 13665, 450, 1565, 123031, 5320, 118169, 2582, 104353, 22827, 16993, 93385, 11, 3721, 5641, 101488, 115822, 111127, 1645, 114608, 81438, 88787, 103594, 68945, 379, 102855, 107172, 9637, 78413, 48320, 627, 2654, 3444, 198, 2654, 3444, 120239, 1609, 735, 12328, 17553, 79919, 3845, 64730, 102234, 2438, 116662, 108398, 77211, 3457, 104031, 273, 342, 1468, 115763, 71344, 4265, 64, 15627, 17106, 103485, 13054, 100923, 111474, 110112, 109551, 275, 101177, 268, 13665, 450, 1565, 11, 109551, 329, 295, 102645, 220, 23, 13, 104032, 459, 111853, 627, 15000, 285, 1871, 21585, 113499, 101165, 2467, 3444, 105364, 80085, 519, 3862, 112477, 101770, 118743, 264, 3742, 65544, 1645, 90513, 259, 1394, 12574, 101588, 33054, 101875, 627, 33, 12273, 14925, 2582]}\n"
     ]
    }
   ],
   "source": [
    "# Veri setinden bir Ã¶rnek yazdÄ±r\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying chat template to train dataset (num_proc=2):   0%|          | 0/2324213 [00:03<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py\", line 3495, in _map_single\n    for i, example in iter_outputs(shard_iterable):\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py\", line 3469, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py\", line 3392, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Mehmet\\Desktop\\Denizhan\\unsloth_compiled_cache\\UnslothSFTTrainer.py\", line 706, in <lambda>\n    lambda ex: {key: ex[key][: args.max_seq_length] for key in [\"input_ids\", \"attention_mask\"]},\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Mehmet\\Desktop\\Denizhan\\unsloth_compiled_cache\\UnslothSFTTrainer.py\", line 706, in <dictcomp>\n    lambda ex: {key: ex[key][: args.max_seq_length] for key in [\"input_ids\", \"attention_mask\"]},\n                     ~~^^^^^\n  File \"C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\formatting\\formatting.py\", line 280, in __getitem__\n    value = self.data[key]\n            ~~~~~~~~~^^^^^\nKeyError: 'attention_mask'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bfloat16_supported   \u001b[38;5;66;03m# Unslothâ€™tan bfloat16 desteÄŸini kontrol eden bir fonksiyon alÄ±r\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# EÄŸitim sÃ¼recini baÅŸlatmak iÃ§in trainer nesnesi oluÅŸturma\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# Modeli eÄŸitmek iÃ§in SFTTrainer nesnesi oluÅŸturur\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# EÄŸitilecek model (Ã¶nceki adÄ±mlarda LoRA ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ)\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Modelin tokenizerâ€™Ä± (metni tokenâ€™lara Ã§evirir)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# EÄŸitim iÃ§in kullanÄ±lacak veri seti (Alpaca formatÄ±nda iÅŸlenmiÅŸ)\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Veri setindeki hangi sÃ¼tunun kullanÄ±lacaÄŸÄ±: \"text\" (biÃ§imlendirilmiÅŸ promptâ€™lar)\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Maksimum dizi uzunluÄŸu (Ã¶nceki adÄ±mda 2048 olarak tanÄ±mlÄ±, uzun baÄŸlamlar iÃ§in yeterli)\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_num_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Veri iÅŸleme iÃ§in kullanÄ±lacak iÅŸlemci sayÄ±sÄ±: 4 (RTX A5000â€™in gÃ¼cÃ¼yle paralel iÅŸleme artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in daha hÄ±zlÄ±)\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#packing=False,                       # Veri paketlemeyi etkinleÅŸtirir; kÄ±sa diziler iÃ§in 5x hÄ±z artÄ±ÅŸÄ± saÄŸlar, bÃ¼yÃ¼k veri setinde faydalÄ±\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# EÄŸitim argÃ¼manlarÄ±nÄ± tanÄ±mlar\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Her cihaz (GPU) iÃ§in eÄŸitim batch boyutu: 8 (24 GB VRAM ile mÃ¼mkÃ¼n, performans artÄ±rÄ±ldÄ±)\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gradyan biriktirme adÄ±mlarÄ±: 4 (efektif batch sizeâ€™Ä± 8*4=32 yapar, bellek yÃ¶netimi iÃ§in)\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Ã–ÄŸrenme oranÄ± Ä±sÄ±nma adÄ±mlarÄ±: 100 (bÃ¼yÃ¼k veri seti iÃ§in daha yavaÅŸ ve stabil Ä±sÄ±nma, 5â€™ten artÄ±rÄ±ldÄ±)\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Tam bir epoch iÃ§in eÄŸitim (yorum satÄ±rÄ±, ÅŸu anda kullanÄ±lmÄ±yor, veri seti bÃ¼yÃ¼k olduÄŸu iÃ§in adÄ±m bazlÄ± tercih ediliyor)\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#max_steps=1000,                 # Toplam eÄŸitim adÄ±mÄ± sayÄ±sÄ±: 1000 (60â€™tan artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in yeterli Ã¶ÄŸrenme saÄŸlamak iÃ§in)\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Ã–ÄŸrenme oranÄ±: 0.0002 (modelin ne kadar hÄ±zlÄ± Ã¶ÄŸreneceÄŸini belirler, bÃ¼yÃ¼k veri setinde uygun)\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Float16 kullanÄ±mÄ±: bfloat16 desteklenmiyorsa True (RTX A5000â€™de bfloat16 destekleniyor)\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_bfloat16_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Bfloat16 kullanÄ±mÄ±: destekleniyorsa True (Ampere mimarisi iÃ§in RTX A5000â€™de etkin, daha verimli)\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Log kayÄ±t sÄ±klÄ±ÄŸÄ±: her 10 adÄ±mda bir (1â€™den artÄ±rÄ±ldÄ±, bÃ¼yÃ¼k veri setinde log sÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r)\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Optimize edici: 8-bit AdamW (bellek verimli bir versiyon, deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# AÄŸÄ±rlÄ±k Ã§Ã¼rÃ¼mesi: 0.01 (overfittingâ€™i Ã¶nlemek iÃ§in regularization, deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Ã–ÄŸrenme oranÄ± zamanlayÄ±cÄ±sÄ±: doÄŸrusal (lineer bir ÅŸekilde azalÄ±r, deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3407\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Rastgele tohum: 3407 (tekrarlanabilirlik iÃ§in, deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHECKPOINT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Ã‡Ä±ktÄ±larÄ±n kaydedileceÄŸi dizin: \"CHECKPOINT_DIR\" (deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# EÄŸitim raporlama: \"none\" (WandB gibi araÃ§lar kullanÄ±lmayacak, deÄŸiÅŸmedi)\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# EÄŸitim argÃ¼manlarÄ±nÄ± tamamlar\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m)\u001b[49m                                       \u001b[38;5;66;03m# Trainer nesnesini oluÅŸturur\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\trainer.py:203\u001b[0m, in \u001b[0;36m_backwards_compatible_trainer.<locals>.new_init\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mehmet\\Desktop\\Denizhan\\unsloth_compiled_cache\\UnslothSFTTrainer.py:933\u001b[0m, in \u001b[0;36mUnslothSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[0m\n\u001b[0;32m    930\u001b[0m fix_untrained_tokens(model, tokenizer, train_dataset, IGNORED_TOKENIZER_NAMES, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-16\u001b[39m)\n\u001b[0;32m    931\u001b[0m fix_zero_training_loss(model, tokenizer, train_dataset)\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneftune_hook_handle\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_hook_handle\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mehmet\\Desktop\\Denizhan\\unsloth_compiled_cache\\UnslothSFTTrainer.py:450\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[0;32m    448\u001b[0m preprocess_dataset \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_prepare_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess_dataset:\n\u001b[1;32m--> 450\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m         packing \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mpacking \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_packing\n",
      "File \u001b[1;32mc:\\Users\\Mehmet\\Desktop\\Denizhan\\unsloth_compiled_cache\\UnslothSFTTrainer.py:705\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer._prepare_dataset\u001b[1;34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[0m\n\u001b[0;32m    701\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    702\u001b[0m         pack_examples, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fn_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: args\u001b[38;5;241m.\u001b[39mmax_seq_length}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmap_kwargs\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# For Liger kernel, ensure only input_ids is present\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39muse_liger:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3171\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3165\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3167\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3168\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3169\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[0;32m   3173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\utils\\py_utils.py:718\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mKeyError\u001b[0m: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma\n",
    "from trl import SFTTrainer              # TRL kÃ¼tÃ¼phanesinden SFTTrainerâ€™Ä± iÃ§e aktarÄ±r, denetimli ince ayar iÃ§in kullanÄ±lÄ±r\n",
    "from transformers import TrainingArguments  # Hugging Face Transformersâ€™tan eÄŸitim argÃ¼manlarÄ±nÄ± iÃ§e aktarÄ±r\n",
    "from unsloth import is_bfloat16_supported   # Unslothâ€™tan bfloat16 desteÄŸini kontrol eden bir fonksiyon alÄ±r\n",
    "\n",
    "# EÄŸitim sÃ¼recini baÅŸlatmak iÃ§in trainer nesnesi oluÅŸturma\n",
    "trainer = SFTTrainer(                   # Modeli eÄŸitmek iÃ§in SFTTrainer nesnesi oluÅŸturur\n",
    "    model=model,                        # EÄŸitilecek model (Ã¶nceki adÄ±mlarda LoRA ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ)\n",
    "    tokenizer=tokenizer,                # Modelin tokenizerâ€™Ä± (metni tokenâ€™lara Ã§evirir)\n",
    "    train_dataset=dataset,              # EÄŸitim iÃ§in kullanÄ±lacak veri seti (Alpaca formatÄ±nda iÅŸlenmiÅŸ)\n",
    "    dataset_text_field=None,          # Veri setindeki hangi sÃ¼tunun kullanÄ±lacaÄŸÄ±: \"text\" (biÃ§imlendirilmiÅŸ promptâ€™lar)\n",
    "    max_seq_length=2048,                # Maksimum dizi uzunluÄŸu (Ã¶nceki adÄ±mda 2048 olarak tanÄ±mlÄ±, uzun baÄŸlamlar iÃ§in yeterli)\n",
    "    dataset_num_proc=2,                 # Veri iÅŸleme iÃ§in kullanÄ±lacak iÅŸlemci sayÄ±sÄ±: 4 (RTX A5000â€™in gÃ¼cÃ¼yle paralel iÅŸleme artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in daha hÄ±zlÄ±)\n",
    "    #packing=False,                       # Veri paketlemeyi etkinleÅŸtirir; kÄ±sa diziler iÃ§in 5x hÄ±z artÄ±ÅŸÄ± saÄŸlar, bÃ¼yÃ¼k veri setinde faydalÄ±\n",
    "    args=TrainingArguments(             # EÄŸitim argÃ¼manlarÄ±nÄ± tanÄ±mlar\n",
    "        per_device_train_batch_size=12,  # Her cihaz (GPU) iÃ§in eÄŸitim batch boyutu: 8 (24 GB VRAM ile mÃ¼mkÃ¼n, performans artÄ±rÄ±ldÄ±)\n",
    "        gradient_accumulation_steps=4,  # Gradyan biriktirme adÄ±mlarÄ±: 4 (efektif batch sizeâ€™Ä± 8*4=32 yapar, bellek yÃ¶netimi iÃ§in)\n",
    "        warmup_steps=100,               # Ã–ÄŸrenme oranÄ± Ä±sÄ±nma adÄ±mlarÄ±: 100 (bÃ¼yÃ¼k veri seti iÃ§in daha yavaÅŸ ve stabil Ä±sÄ±nma, 5â€™ten artÄ±rÄ±ldÄ±)\n",
    "        num_train_epochs=2,           # Tam bir epoch iÃ§in eÄŸitim (yorum satÄ±rÄ±, ÅŸu anda kullanÄ±lmÄ±yor, veri seti bÃ¼yÃ¼k olduÄŸu iÃ§in adÄ±m bazlÄ± tercih ediliyor)\n",
    "        #max_steps=1000,                 # Toplam eÄŸitim adÄ±mÄ± sayÄ±sÄ±: 1000 (60â€™tan artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in yeterli Ã¶ÄŸrenme saÄŸlamak iÃ§in)\n",
    "        learning_rate=2e-4,             # Ã–ÄŸrenme oranÄ±: 0.0002 (modelin ne kadar hÄ±zlÄ± Ã¶ÄŸreneceÄŸini belirler, bÃ¼yÃ¼k veri setinde uygun)\n",
    "        fp16=not is_bfloat16_supported(),  # Float16 kullanÄ±mÄ±: bfloat16 desteklenmiyorsa True (RTX A5000â€™de bfloat16 destekleniyor)\n",
    "        bf16=is_bfloat16_supported(),   # Bfloat16 kullanÄ±mÄ±: destekleniyorsa True (Ampere mimarisi iÃ§in RTX A5000â€™de etkin, daha verimli)\n",
    "        logging_steps=10,               # Log kayÄ±t sÄ±klÄ±ÄŸÄ±: her 10 adÄ±mda bir (1â€™den artÄ±rÄ±ldÄ±, bÃ¼yÃ¼k veri setinde log sÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r)\n",
    "        optim=\"adamw_8bit\",             # Optimize edici: 8-bit AdamW (bellek verimli bir versiyon, deÄŸiÅŸmedi)\n",
    "        weight_decay=0.01,              # AÄŸÄ±rlÄ±k Ã§Ã¼rÃ¼mesi: 0.01 (overfittingâ€™i Ã¶nlemek iÃ§in regularization, deÄŸiÅŸmedi)\n",
    "        lr_scheduler_type=\"linear\",     # Ã–ÄŸrenme oranÄ± zamanlayÄ±cÄ±sÄ±: doÄŸrusal (lineer bir ÅŸekilde azalÄ±r, deÄŸiÅŸmedi)\n",
    "        seed=3407,                      # Rastgele tohum: 3407 (tekrarlanabilirlik iÃ§in, deÄŸiÅŸmedi)\n",
    "        output_dir=CHECKPOINT_DIR,      # Ã‡Ä±ktÄ±larÄ±n kaydedileceÄŸi dizin: \"CHECKPOINT_DIR\" (deÄŸiÅŸmedi)\n",
    "        report_to=\"none\",               # EÄŸitim raporlama: \"none\" (WandB gibi araÃ§lar kullanÄ±lmayacak, deÄŸiÅŸmedi)\n",
    "    ),                                  # EÄŸitim argÃ¼manlarÄ±nÄ± tamamlar\n",
    ")                                       # Trainer nesnesini oluÅŸturur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(f\"{CHECKPOINT_DIR}/final_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 9. EÄŸitimi BaÅŸlat\n",
    "# ----------------------------------------------\n",
    "try:                                    # Hata yakalama bloÄŸu baÅŸlatÄ±r, checkpoint ile devam etmeyi dener\n",
    "    print(\"Checkpoint aranÄ±yor...\")     # KullanÄ±cÄ±ya checkpointâ€™in arandÄ±ÄŸÄ±nÄ± bildirir\n",
    "    trainer.train(resume_from_checkpoint=True)  # EÄŸitime son checkpointâ€™ten devam etmeyi dener (Ã¶nceki bir eÄŸitim varsa)\n",
    "except Exception as e:                  # EÄŸer bir hata oluÅŸursa (Ã¶rneÄŸin, checkpoint bulunamazsa) bu blok Ã§alÄ±ÅŸÄ±r\n",
    "    print(f\"Hata: {str(e)[:200]}...\\nYeni eÄŸitim baÅŸlatÄ±lÄ±yor...\")  # HatanÄ±n ilk 200 karakterini yazdÄ±rÄ±r ve yeni eÄŸitime geÃ§eceÄŸini bildirir\n",
    "    trainer.train()                     # SÄ±fÄ±rdan yeni bir eÄŸitim baÅŸlatÄ±r (checkpoint olmadan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# 10. Modeli Kaydet\n",
    "# ----------------------------------------------\n",
    "model.save_pretrained(f\"{CHECKPOINT_DIR}/final_model2\")  # EÄŸitilmiÅŸ modeli belirtilen dizine kaydeder (CHECKPOINT_DIR/final_model)\n",
    "tokenizer.save_pretrained(f\"{CHECKPOINT_DIR}/final_model2\")  # Tokenizerâ€™Ä± aynÄ± dizine kaydeder\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 11. Opsiyonel: EÄŸitim Ä°lerlemesini Ä°zleme ve Log Kaydetme\n",
    "# ----------------------------------------------\n",
    "trainer.save_model(f\"{CHECKPOINT_DIR}/final_checkpoin2t\")  # EÄŸitim sÄ±rasÄ±nda ara checkpointâ€™leri kaydet (opsiyonel, bÃ¼yÃ¼k veri seti iÃ§in faydalÄ±)\n",
    "print(f\"EÄŸitim tamamlandÄ±. Model ÅŸu adreste kaydedildi: {CHECKPOINT_DIR}/final_mode2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
