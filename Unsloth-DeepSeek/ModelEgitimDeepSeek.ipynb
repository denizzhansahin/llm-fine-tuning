{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth_zoo in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2025.2.7)\n",
      "Requirement already satisfied: torch in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (2.6.0+cu124)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (24.1)\n",
      "Requirement already satisfied: tyro in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.9.16)\n",
      "Collecting transformers>=4.46.1 (from unsloth_zoo)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (1.26.4)\n",
      "Collecting accelerate>=0.34.1 (from unsloth_zoo)\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.29.1)\n",
      "Requirement already satisfied: hf_transfer in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.1.9)\n",
      "Requirement already satisfied: cut_cross_entropy in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (25.1.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (10.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate>=0.34.1->unsloth_zoo) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->unsloth_zoo) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers>=4.46.1->unsloth_zoo) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.46.1->unsloth_zoo)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch->unsloth_zoo) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.17.0)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.32.1\n",
      "    Uninstalling accelerate-0.32.1:\n",
      "      Successfully uninstalled accelerate-0.32.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "Successfully installed accelerate-1.4.0 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"senin token\"  # Hugging Face üzerinden ince ayar için kullanacağımız modeli indirmek için Hugging Face tokenı gerekmektedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:24<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\Desktop\\Denizhan2\\deepseek-14B does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kütüphaneyi içe aktarma\n",
    "from unsloth import FastLanguageModel  # Unsloth kütüphanesinden hızlı model yükleme sınıfını içe aktarır, büyük dil modellerini optimize eder\n",
    "\n",
    "# PyTorch kütüphanesini içe aktarma\n",
    "import torch                           # PyTorch'u tensör işlemleri ve model hesaplamaları için içe aktarır\n",
    "\n",
    "# Maksimum dizi uzunluğu ayarı\n",
    "max_seq_length = 2048                  # Modelin bir seferde işleyebileceği maksimum token sayısını 2048 olarak tanımlar, RoPE Scaling ile otomatik ölçeklenir\n",
    "\n",
    "# Veri tipi ayarı\n",
    "#dtype = None                           # Veri tipini otomatik algılamaya bırakır; Tesla T4/V100 için Float16, Ampere+ için Bfloat16 seçilir\n",
    "dtype = torch.bfloat16 #A5000 ekran kartı için bunu seçtik.\n",
    "\n",
    "# 4-bit kuantizasyon ayarı\n",
    "load_in_4bit = True                    # Modeli 4-bit kuantizasyon ile yükler, bu bellek kullanımını azaltır ve performansı korur (False yapılırsa tam hassasiyet kullanılır)\n",
    "\n",
    "# 4-bit önceden kuantize edilmiş modellerin listesi\n",
    "fourbit_models = [                     # Unsloth’un desteklediği, 4-bit kuantize edilmiş modellerin listesi; hızlı indirme ve bellek tasarrufu sağlar\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 8B modeli, 15 trilyon token ile eğitilmiş, 2 kat hızlı\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  # Llama-3.1 8B’nin talimatlara özel sürümü\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",     # Llama-3.1 70B modeli\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # Llama-3.1 405B modeli, 4-bit olarak yükleniyor\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Mistral Nemo temel modeli, 12B, 2 kat hızlı\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",  # Mistral Nemo’nun talimat sürümü\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral 7B v0.3, 2 kat hızlı\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",   # Mistral 7B v0.3 talimat sürümü\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 mini, talimatlara özel, 2 kat hızlı\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",        # Phi-3 orta boy, 4k token kapasiteli\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",             # Gemma 2 9B modeli\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2 27B modeli, 2 kat hızlı\n",
    "]  # Daha fazla model için: https://huggingface.co/unsloth\n",
    "model_path = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\deepseek-14B\"\n",
    "\n",
    "# Modeli ve tokenizer'ı önceden eğitilmiş olarak yükleme\n",
    "model, tokenizer = FastLanguageModel.from_pretrained( \n",
    "   # force_download=True,\n",
    "      # Modeli ve tokenizer'ı yükler, Unsloth optimizasyonlarıyla\n",
    "    model_name=model_path,   # Yüklenecek modelin adı: LLaMA-3.1 8B (8 milyar parametre)\n",
    "    max_seq_length=max_seq_length,            # Maksimum dizi uzunluğunu 2048 olarak ayarlar\n",
    "    dtype=dtype,                              # Veri tipini otomatik algılamaya bırakır (Tesla T4 için Float16 olur)\n",
    "    load_in_4bit=load_in_4bit,                # 4-bit kuantizasyonu etkinleştirir, belleği optimize eder\n",
    "    #token = \"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\",                       # Eğer kısıtlı erişimli bir model kullanıyorsanız Hugging Face token’ı eklenir (burada pasif)\n",
    ")  # Model ve tokenizer nesnelerini döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR =  \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\" #Model verilerini kayıt edeceğiniz klasörü seçin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Modeli LoRA ile yapılandırma\n",
    "model = FastLanguageModel.get_peft_model(  # Modeli LoRA ile optimize edilmiş bir PEFT (Parameter-Efficient Fine-Tuning) modeline dönüştürür\n",
    "    model,                                 # LoRA'nın uygulanacağı ana model (önceki adımda yüklenen model)\n",
    "    r=32,                                  # LoRA'nın rank değeri: 16’dan 32’ye artırıldı (RTX A5000’in 24 GB VRAM’i daha yüksek rank için yeterli, daha iyi performans için) eskisi ise 16\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # LoRA'nın uygulanacağı Transformer katmanları (dikkat mekanizması ve feed-forward katmanları, değişmedi)\n",
    "    lora_alpha=32,                         # LoRA'nın ölçeklendirme faktörü: 16’dan 32’ye artırıldı (r ile uyumlu, daha güçlü öğrenme için) eskisi ise 16\n",
    "    lora_dropout=0.05,                     # LoRA için dropout oranı: 0’dan 0.05’e artırıldı (büyük veri setinde overfitting riskine karşı) eskisi ise 0\n",
    "    bias=\"none\",                           # Bias ayarı: \"none\" (değişmedi, optimize edilmiş bir seçenek)\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Gradient checkpointing ayarı: \"unsloth\" (değişmedi, VRAM’i %30 azaltır, 2 kat büyük batch size sağlar)\n",
    "    random_state=3407,                     # Rastgele durum tohumu: 3407 (değişmedi, tekrarlanabilirlik için sabit)\n",
    "    use_rslora=True,                       # Rank Stabilized LoRA (RSLoRA) kullanımı: False’tan True’ya değiştirildi (stabiliteyi artırır, büyük veri setlerinde faydalı) eskisi ise False\n",
    "    loftq_config=None,                     # LoftQ (Low-Rank Factorized Quantization) yapılandırması: None (değişmedi, ihtiyaç yoksa ek optimizasyon gerekmez)\n",
    ")                                          # LoRA ile yapılandırılmış yeni modeli döndürür"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatı için prompt şablonu tanımlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # Üç bölümden oluşan bir şablon: talimat (instruction), giriş (input) ve cevap (response) yer tutucuları içerir\n",
    "\n",
    "# EOS_TOKEN tanımlama\n",
    "EOS_TOKEN = tokenizer.eos_token  # Tokenizer'dan EOS (End of Sequence) token'ını alır, metnin sonunu işaretler\n",
    "\n",
    "# Veri setini biçimlendirme fonksiyonu\n",
    "def formatting_prompts_func(examples):  # Veri setindeki örnekleri Alpaca formatına dönüştürmek için bir fonksiyon\n",
    "    instructions = examples[\"instruction\"]  # Veri setindeki \"instruction\" sütununu alır (talimatlar)\n",
    "    inputs       = examples[\"input\"]        # Veri setindeki \"input\" sütununu alır (girişler)\n",
    "    outputs      = examples[\"output\"]       # Veri setindeki \"output\" sütununu alır (cevaplar)\n",
    "    texts = []                              # Biçimlendirilmiş metinleri saklamak için boş bir liste oluşturur\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):  # Her bir talimat, giriş ve cevap üçlüsünü eşleştirir\n",
    "        # Alpaca şablonunu doldurur ve EOS_TOKEN ekler, yoksa üretim sonsuza dek devam eder!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN  # Şablonu doldurur ve metnin sonuna EOS token’ı ekler\n",
    "        texts.append(text)                  # Biçimlendirilmiş metni listeye ekler\n",
    "    return { \"text\" : texts, }              # \"text\" anahtarıyla biçimlendirilmiş metinleri bir sözlük olarak döndürür\n",
    "pass                                       # Boş bir \"pass\" ifadesi (gereksiz, fonksiyon zaten tamamlandı)\n",
    "\n",
    "# Veri setini yükleme ve işleme\n",
    "from datasets import load_dataset     \n",
    "from datasets import load_from_disk\n",
    "  # Hugging Face’in datasets kütüphanesini içe aktarır\n",
    "dataset = load_dataset(\"json\", data_files=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\alpaca_format.json\")[\"train\"]  # JSON dosyasından veri setini yükler ve \"train\" bölümünü alır\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)  # Veri setine biçimlendirme fonksiyonunu toplu (batched) şekilde uygular\n",
    "\n",
    "#tokenized_dataset = load_from_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset\")\n",
    "\n",
    "# Eğitim için dataset'i ayarla\n",
    "#dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def add_attention_mask(example):\n",
    "    example[\"attention_mask\"] = [1] * len(example[\"input_ids\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_attention_mask, num_proc=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=4): 100%|██████████| 4763907/4763907 [48:03<00:00, 1652.25 examples/s]\n",
      "Packing train dataset (num_proc=4): 100%|██████████| 4763907/4763907 [1:51:45<00:00, 710.45 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri içe aktarma\n",
    "from trl import SFTTrainer              # TRL kütüphanesinden SFTTrainer’ı içe aktarır, denetimli ince ayar için kullanılır\n",
    "from transformers import TrainingArguments  # Hugging Face Transformers’tan eğitim argümanlarını içe aktarır\n",
    "from unsloth import is_bfloat16_supported   # Unsloth’tan bfloat16 desteğini kontrol eden bir fonksiyon alır\n",
    "\n",
    "# Eğitim sürecini başlatmak için trainer nesnesi oluşturma\n",
    "trainer = SFTTrainer(                   # Modeli eğitmek için SFTTrainer nesnesi oluşturur\n",
    "    model=model,                        # Eğitilecek model (önceki adımlarda LoRA ile yapılandırılmış)\n",
    "    tokenizer=tokenizer,                # Modelin tokenizer’ı (metni token’lara çevirir)\n",
    "    train_dataset=dataset,              # Eğitim için kullanılacak veri seti (Alpaca formatında işlenmiş)\n",
    "    dataset_text_field=\"text\",          # Veri setindeki hangi sütunun kullanılacağı: \"text\" (biçimlendirilmiş prompt’lar)\n",
    "    max_seq_length=2048,                # Maksimum dizi uzunluğu (önceki adımda 2048 olarak tanımlı, uzun bağlamlar için yeterli)\n",
    "    dataset_num_proc=4,                 # Veri işleme için kullanılacak işlemci sayısı: 4 (RTX A5000’in gücüyle paralel işleme artırıldı, 5 milyon satır için daha hızlı)\n",
    "    packing=True,                       # Veri paketlemeyi etkinleştirir; kısa diziler için 5x hız artışı sağlar, büyük veri setinde faydalı\n",
    "    args=TrainingArguments(             # Eğitim argümanlarını tanımlar\n",
    "        per_device_train_batch_size=8,  # Her cihaz (GPU) için eğitim batch boyutu: 8 (24 GB VRAM ile mümkün, performans artırıldı)\n",
    "        gradient_accumulation_steps=4,  # Gradyan biriktirme adımları: 4 (efektif batch size’ı 8*4=32 yapar, bellek yönetimi için)\n",
    "        warmup_steps=100,               # Öğrenme oranı ısınma adımları: 100 (büyük veri seti için daha yavaş ve stabil ısınma, 5’ten artırıldı)\n",
    "        num_train_epochs=2,           # Tam bir epoch için eğitim (yorum satırı, şu anda kullanılmıyor, veri seti büyük olduğu için adım bazlı tercih ediliyor)\n",
    "        max_steps=1500,                 # Toplam eğitim adımı sayısı: 1000 (60’tan artırıldı, 5 milyon satır için yeterli öğrenme sağlamak için)\n",
    "        learning_rate=2e-4,             # Öğrenme oranı: 0.0002 (modelin ne kadar hızlı öğreneceğini belirler, büyük veri setinde uygun)\n",
    "        fp16=not is_bfloat16_supported(),  # Float16 kullanımı: bfloat16 desteklenmiyorsa True (RTX A5000’de bfloat16 destekleniyor)\n",
    "        bf16=is_bfloat16_supported(),   # Bfloat16 kullanımı: destekleniyorsa True (Ampere mimarisi için RTX A5000’de etkin, daha verimli)\n",
    "        logging_steps=10,               # Log kayıt sıklığı: her 10 adımda bir (1’den artırıldı, büyük veri setinde log sıklığını azaltır)\n",
    "        optim=\"adamw_8bit\",             # Optimize edici: 8-bit AdamW (bellek verimli bir versiyon, değişmedi)\n",
    "        weight_decay=0.01,              # Ağırlık çürümesi: 0.01 (overfitting’i önlemek için regularization, değişmedi)\n",
    "        lr_scheduler_type=\"linear\",     # Öğrenme oranı zamanlayıcısı: doğrusal (lineer bir şekilde azalır, değişmedi)\n",
    "        seed=3407,                      # Rastgele tohum: 3407 (tekrarlanabilirlik için, değişmedi)\n",
    "        output_dir=CHECKPOINT_DIR,      # Çıktıların kaydedileceği dizin: \"CHECKPOINT_DIR\" (değişmedi)\n",
    "        report_to=\"none\",               # Eğitim raporlama: \"none\" (WandB gibi araçlar kullanılmayacak, değişmedi)\n",
    "    ),                                  # Eğitim argümanlarını tamamlar\n",
    ")                                       # Trainer nesnesini oluşturur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (42/42 shards): 100%|██████████| 2518276/2518276 [00:12<00:00, 201447.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Token’lanmış veri setini kaydet\n",
    "trainer.train_dataset.save_to_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset2_atten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint aranıyor...\n",
      "Hata: No valid checkpoint found in output directory (C:\\Users\\Mehmet\\Desktop\\Denizhan2\\model_egitim)...\n",
      "Yeni eğitim başlatılıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,518,276 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 32 | Total steps = 1,500\n",
      " \"-____-\"     Number of trainable parameters = 137,625,600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 39:49:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.792300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.743100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.630500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# 9. Eğitimi Başlat\n",
    "# ----------------------------------------------\n",
    "try:                                    # Hata yakalama bloğu başlatır, checkpoint ile devam etmeyi dener\n",
    "    print(\"Checkpoint aranıyor...\")     # Kullanıcıya checkpoint’in arandığını bildirir\n",
    "    trainer.train(resume_from_checkpoint=True)  # Eğitime son checkpoint’ten devam etmeyi dener (önceki bir eğitim varsa)\n",
    "except Exception as e:                  # Eğer bir hata oluşursa (örneğin, checkpoint bulunamazsa) bu blok çalışır\n",
    "    print(f\"Hata: {str(e)[:200]}...\\nYeni eğitim başlatılıyor...\")  # Hatanın ilk 200 karakterini yazdırır ve yeni eğitime geçeceğini bildirir\n",
    "    trainer.train()                     # Sıfırdan yeni bir eğitim başlatır (checkpoint olmadan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model eğitimini durdurmak gerekiyordu çünkü 26 saat istedi.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 step için 26 saat verdi ve 137,625,600 parametre eğitilecek. Bunu hafta sonu 2000 step için 52 saat+3 saat tokenizer olarak yaptığımızda 274 milyon parametre eğitilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:14<00:00,  1.21s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatı için prompt şablonu tanımlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # Üç bölümden oluşan bir şablon: talimat (instruction), giriş (input) ve cevap (response) yer tutucuları içerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Fenerbahçe'nin son maçlarının sonuçlarını listele ve her maç için kısa bir açıklama yap.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Fenerbahçe'nin son maçları şunlardır:\n",
      "\n",
      " 19 Eylül 2014 - İstanbul - Fenerbahçe - Galatasaray - 3-1 - Süper Kupa\n",
      " 19 Eylül 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 2-1 - Süper Lig\n",
      " 27 Eylül 2014 - İstanbul - Fenerbahçe - Gaziantepspor - 2-0 - Süper Lig\n",
      " 11 Ekim 2014 - İstanbul - Fenerbahçe - Trabzonspor - 1-0 - Süper Lig\n",
      " 15 Ekim 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 3-1 - Süper Lig\n",
      " 19 Ekim 2014 - İstanbul - Fenerbahçe - Trabzonspor - 3-0 - Süper Lig\n",
      " 26 Ekim 2014 - İstanbul - Fenerbahçe - Sivasspor - 1-0 - Süper Lig\n",
      " 2 Kasım 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 10 Kasım 2014 - İstanbul - Fenerbahçe - Galatasaray - 2-1 - Süper Lig\n",
      " 23 Kasım 2014 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 26 Kasım 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 8 Aralık 2014 - İstanbul - Fenerbahçe - Adana Demirspor - 2-0 - Süper Lig\n",
      " 13 Aralık 2014 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 20 Aralık 2014 - İstanbul - Fenerbahçe - Sivasspor - 2-1 - Süper Lig\n",
      " 27 Aralık 2014 - İstanbul - Fenerbahçe - Kasımpaşa - 1-0 - Süper Lig\n",
      " 1 Ocak 2015 - İstanbul - Fenerbahçe - Trabzonspor - 1-1 - Süper Lig\n",
      " 4 Ocak 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 10 Ocak 2015 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 25 Ocak 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 28 Ocak 2015 - İstanbul - Fenerbahçe - Sivasspor - 2-0 - Süper Lig\n",
      " 11 Şubat 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 14 Şubat 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 3-0 - Süper Lig\n",
      " 21 Şubat 2015 - İstanbul - Fenerbahçe - Trabzonspor - 2-0 - Süper Lig\n",
      " 28 Şubat 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 12 Mart 2015 - İstanbul - Fenerbahçe - Sivasspor - 1-1 - Süper Lig\n",
      " 16 Mart 2015 - İstanbul - Fenerbahçe - Trabzonspor - 1-1 - Süper Lig\n",
      " 22 Mart 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 2-1 - Süper Lig\n",
      " 26 Mart 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-1 - Süper Lig\n",
      " 29 Mart 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 3-0 - Süper Lig\n",
      " 12 Nisan 2015 - İstanbul - Fenerbahçe - Kasımpaşa - 1-0 - Süper Lig\n",
      " 15 Nisan 2015 - İstanbul - Fenerbahçe - Trabzonspor - 3-2 - Süper Lig\n",
      " 22 Nisan 2015 - İstanbul - F\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Fenerbahçe'nin son maçlarının sonuçlarını listele ve her maç için kısa bir açıklama yap.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Türkiye Süper Lig futbol takımları hakında bilgi ver.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Türkiye Süper Lig, 1959'da kurulan Türk futbol ligi. Türkiye'de oynanan profesyonel futbol ligidir. Türkiye'de 1959-60 sezonunda başlamıştır. Süper Lig, Türkiye Futbol Federasyonu tarafından düzenlenir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en üst düzey ligdir. Süper Lig, Türkiye'de oynanan en\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Türkiye Süper Lig futbol takımları hakında bilgi ver.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denizhan Şahin kimdir? Denizhan Şahin kaç yaşında? Denizhan Şahin nereli?\n",
      "Denizhan Şahin kimdir? Denizhan Şahin kaç yaşında? Denizhan Şahin nereli? Denizhan Şahin biyografisi, Denizhan Şahin hayatı hakkında bilgiler...\n",
      "Denizhan Şahin kimdir?\n",
      "Denizhan Şahin (d. 1992, İstanbul), Türk oyuncu. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin kaç yaşında?\n",
      "Denizhan Şahin 1992 yılında doğduğu için 2022 yılında 30 yaşında.\n",
      "Denizhan Şahin nereli?\n",
      "Denizhan Şahin İstanbul'da doğdu.\n",
      "Denizhan Şahin biyografisi\n",
      "Denizhan Şahin, 1992 yılında İstanbul'da doğdu. İlk ve orta öğrenimini İstanbul'da tamamladı. Liseyi İstanbul'da tamamladıktan sonra İstanbul Üniversitesi, İletişim Fakültesi'nde eğitim gördü. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin hayatı hakkında bilgiler\n",
      "Denizhan Şahin, 1992 yılında İstanbul'da doğdu. İlk ve orta öğrenimini İstanbul'da tamamladı. Liseyi İstanbul'da tamamladıktan sonra İstanbul Üniversitesi, İletişim Fakültesi'nde eğitim gördü. 2015 yılında ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Çiğdem\" karakteri ile yaşayan Şahin, \"Gönül Yaralanışı\", \"Kardeşler\", \"Kara Korsanlar\", \"Şimşekler Ateşe Karşı\", \"Sevgili Evren\", \"Kara Ağa\", \"Kızıl Yol\", \"Çocuklar\" ve \"İstanbul'un Oğlu\" gibi dizilerde rol aldı.\n",
      "Denizhan Şahin, \"Kara Korsanlar\", \"Kara Ağa\", \"İstanbul'un Oğlu\" ve \"Çocuklar\" dizilerindeki performansı ile \"En İyi Kadın Oyuncu\" dalında Altın Kelebek Ödülleri'ne aday gösterildi.\n",
      "Denizhan Şahin, 2017 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2018 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2019 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2020 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2021 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2022 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2023 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2024 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2025 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2026 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2027 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2028 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2029 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2030 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2031 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2032 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2033 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2034 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2035 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2036 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2037 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2038 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2039 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2040 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2041 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2042 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2043 yılında \"Kara Korsanlar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödülünü aldı.\n",
      "Denizhan Şahin, 2044 yılında \"Çocuklar\" dizisiyle Altın Kelebek Ödülleri'nde \"En İyi Kadın Oyuncu\" ödül\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Denizhan Şahin kimdir?\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sen hangi yapay zeka modelisin? Kendini tanıt.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Sen hangi yapay zeka modelisin? Kendini tanıt.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek nedir? DeepSeek, yapay zeka ve büyük veri teknolojilerini kullanarak bilgi ve bilgi kaynağı arayıcısı olarak çalışan bir teknoloji şirketidir..DeepSeek, büyük veri teknolojilerini ve yapay zeka algoritmalarını kullanarak bilgi ve bilgi kaynağı arayıcısı olarak çalışan bir teknoloji şirketidir. DeepSeek, verileri analiz ederek, bilgi araması ve analizlerde daha doğru ve verimli sonuçlar elde etmenizi sağlar. Ayrıca, şirket, kullanıcıların ihtiyaçlarına göre özelleştirilmiş veri analizi ve raporlama hizmetleri de sunar. DeepSeek, özellikle finans, sağlık, pazarlama ve güvenlik gibi alanlarda kullanılmaktadır.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "               # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"DeepSeek nedir?\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "                                   # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=2048,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Çıktıyı decode etme ve yazdırma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Üretilen token’ları insan tarafından okunabilir metne çevirir, özel token’ları atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kısım\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Streamer'ı oluştur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # Başlangıç promptunu gösterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\"Denizhan Şahin kimdir?\"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 puanlı bir takım. Benim için 100 puanlı bir takım. Bu takımın önünde bir şey yok. Bu takımın önünde hiçbir şey yok. Benim için 100 puanlı bir takım. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oyn\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Fenerbahçe nasıl bir takım? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.06.2018, 15:41 10.06.2018, 15:41\n",
      "Teknoloji Festivalleri ve Yarışmaları (Teknofest) kapsamında düzenlenen 1. Uçak Mühendisliği Yarışmasında birinci olan Çankaya Üniversitesine ait ekip, 10 Ağustos 2018 tarihinde yapılacak Teknofest İstanbul'da Türkiye'yi temsil edecek. Teknofest İstanbul 2018'te 10 Ağustos 2018 tarihinde başlayacak. 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul 2018'de, 10 Ağustos'ta başlayacak Teknofest İstanbul "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      2\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeknofest için bilgi verebilir misin? \u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate içinde streamer kullan\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Streamer'ı parametre olarak ekle\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1596\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \n\u001b[0;32m   1594\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[1;32m-> 1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \n\u001b[0;32m   1602\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1840\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2220\u001b[0m     )\n\u001b[0;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3218\u001b[0m     outputs,\n\u001b[0;32m   3219\u001b[0m     model_kwargs,\n\u001b[0;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3221\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1043\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[1;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1026\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1040\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[1;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[0;32m    970\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[0;32m    971\u001b[0m X \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(\n\u001b[0;32m    972\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm,\n\u001b[0;32m    973\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     variance \u001b[38;5;241m=\u001b[39m variance,\n\u001b[0;32m    977\u001b[0m )\n\u001b[1;32m--> 978\u001b[0m X, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[0;32m    988\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:218\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[1;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[0;32m    216\u001b[0m RH_Q[:,:,:,:h] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,h:]\n\u001b[0;32m    217\u001b[0m RH_Q[:,:,:,h:] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,:h]\n\u001b[1;32m--> 218\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRH_Q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRH_Q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m Qn \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cos\n\u001b[0;32m    220\u001b[0m Qn\u001b[38;5;241m.\u001b[39maddcmul_(RH_Q, sin)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Teknofest için bilgi verebilir misin? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Türkiye Süper Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Süper Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır. Millî Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Süper Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Sezonlar\n",
      "1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Süper Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Süper Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Süper Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Süper Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Millî Lig adıyla oynanmıştır.\n",
      "\n",
      "Süper Lig, 1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Süper Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Millî Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1972'de kurulan Türkiye'nin en üst düzey futbol ligi. Millî Lig, 1972-1973 sezonundan beri oynanmaktadır. 1984-85 sezonundan bu yana 18 takımın oynadığı ligde, 16 takımın oynadığı sezonlar da vardır. 1999-2000 sezonunda 20 takımla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takımın oynadığı ligde, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir. Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla oynanmıştır.\n",
      "\n",
      "1998-99 sezonu öncesinde Türkiye'de oynanan en üst seviyede profesyonel futbol ligi olan Millî Lig'de oynayan 18 takımdan oluşuyordu. 1998-99 sezonunda ise ligde 18 takım yer almıştı. 2000-01 sezonundan 2011-12 sezonuna kadar Millî Lig'de oynayan 18 takımın, 2012-13 sezonundan beri 16 takımın oynadığı ligde oynanmaktadır. Millî Lig, Türkiye Futbol Federasyonu (TFF) tarafından organize edilmektedir.\n",
      "\n",
      "Millî Lig, 2001-2002 sezonuna kadar Türkiye'nin en üst seviye ligi olan Süper Lig adıyla\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kısım\n",
    "import torch\n",
    "# alpaca_prompt = Yukarıdan kopyalandığını varsayıyorum\n",
    "# Alpaca formatında prompt şablonunun \"instruction\", \"input\", \"response\" alanlarını içerdiğini kabul ediyorum\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "# Modeli tahmin moduna geçirme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference için hazırlar, Unsloth’un 2 kat hızlı yerel tahmin özelliğini etkinleştirir\n",
    "\n",
    "# Streamer'ı oluştur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # Başlangıç promptunu gösterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Girişi hazırlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriş metnini token’lara çevirir\n",
    "    [                                   # Tek bir prompt’u liste içinde işler\n",
    "        alpaca_prompt.format(           # Alpaca şablonunu belirli değerlerle doldurur\n",
    "            \"Türkiye Süper Lig futbol takımları hakında bilgi ver.\",  # Talimat: Maç sonuçlarını listele ve açıklama ekle\n",
    "            \"\",                         # Giriş: Boş bırakıldı, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Çıkış: Boş bırakılır, modelin üretmesi için\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Çıktıyı PyTorch tensör formatında döndürür\n",
    ").to(\"cuda\")                            # Tensörleri CUDA (GPU) belleğine taşır\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin üretir\n",
    "    **inputs,                           # Tokenize edilmiş girişleri modele verir\n",
    "    max_new_tokens=1024,                 # Üretilecek maksimum yeni token sayısı: Daha fazla ayrıntı için 512\n",
    "    use_cache=True                      # Önbellek kullanımını etkinleştirir, hızı artırır\n",
    ")\n",
    "\n",
    "# Generate içinde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'ı parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting Werkzeug>=3.1 (from Flask)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask) (3.1.5)\n",
      "Collecting itsdangerous>=2.2 (from Flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask) (8.1.8)\n",
      "Collecting blinker>=1.9 (from Flask)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=8.1.3->Flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Jinja2>=3.1.2->Flask) (2.1.5)\n",
      "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: Werkzeug, itsdangerous, blinker, Flask\n",
      "Successfully installed Flask-3.1.0 Werkzeug-3.1.3 blinker-1.9.0 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:15<00:00,  1.28s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:40] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:52] \"GET /generate?prompt=Merhaba HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:04:34] \"GET /generate?prompt=Fenerbahçe%20hakkında%20bilgi%20ver. HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:04:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:05:04] \"GET /generate?prompt=Fenerbahçe%20hakkında%20bilgi%20ver HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:05:15] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "        self.current_text = \"\"\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.current_text += text\n",
    "        # Yeni satır veya noktaya göre böl\n",
    "        if '\\n' in text or '. ' in text:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.current_text = \"\"\n",
    "        elif stream_end:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.queue.put(None)  # Bitiş işareti\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                const eventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                eventSource.onmessage = function(e) {\n",
    "                    if (e.data === 'DONE') {\n",
    "                        eventSource.close();\n",
    "                        return;\n",
    "                    }\n",
    "                    output.innerHTML += e.data;\n",
    "                    window.scrollTo(0, document.body.scrollHeight);\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "    \n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        model.generate(\n",
    "            **inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=2048,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        response_queue.put(None)  # İşlem bitti sinyali\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:22<00:00,  1.84s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:41] \"GET /generate?prompt=Fenerbahçe%20nasıl%20bir%20takım? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:07] \"GET /generate?prompt=Süper%20lig%20nasıldır? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:34] \"GET /generate?prompt=Süper%20Lig%20maçları%20skorları%20bu%20hafta HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yönetimi için global değişkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            \n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                // Önceki bağlantıyı kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                currentEventSource.onmessage = function(e) {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        output.innerHTML += e.data;\n",
    "                        window.scrollTo(0, document.body.scrollHeight);\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Önceki işlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:14<00:00,  1.24s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:15:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:15:05] \"GET /generate?prompt=Süper%20Lig%20maçları%20skorları%20bu%20hafta HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'ı global olarak yükle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yönetimi için global değişkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>AI Chat</title>\n",
    "        <style>\n",
    "            :root {\n",
    "                --primary-color: #2563eb;\n",
    "                --user-bg: #3b82f6;\n",
    "                --bot-bg: #f1f5f9;\n",
    "                --text-color: #1e293b;\n",
    "            }\n",
    "\n",
    "            * {\n",
    "                box-sizing: border-box;\n",
    "                font-family: 'Segoe UI', sans-serif;\n",
    "            }\n",
    "\n",
    "            body {\n",
    "                margin: 0;\n",
    "                background: #f8fafc;\n",
    "                height: 100vh;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "            }\n",
    "\n",
    "            .chat-container {\n",
    "                flex: 1;\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                width: 100%;\n",
    "                padding: 20px;\n",
    "                overflow-y: auto;\n",
    "            }\n",
    "\n",
    "            .message {\n",
    "                max-width: 70%;\n",
    "                margin: 10px 0;\n",
    "                padding: 15px 20px;\n",
    "                border-radius: 20px;\n",
    "                animation: fadeIn 0.3s ease-out;\n",
    "                transform-origin: bottom;\n",
    "                position: relative;\n",
    "            }\n",
    "\n",
    "            .user-message {\n",
    "                background: var(--user-bg);\n",
    "                color: white;\n",
    "                margin-left: auto;\n",
    "                border-bottom-right-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .bot-message {\n",
    "                background: var(--bot-bg);\n",
    "                color: var(--text-color);\n",
    "                margin-right: auto;\n",
    "                border-bottom-left-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .typing-indicator {\n",
    "                display: inline-flex;\n",
    "                padding: 10px 15px;\n",
    "                background: var(--bot-bg);\n",
    "                border-radius: 20px;\n",
    "                margin: 5px 0;\n",
    "            }\n",
    "\n",
    "            .dot {\n",
    "                width: 8px;\n",
    "                height: 8px;\n",
    "                margin: 0 3px;\n",
    "                background: #64748b;\n",
    "                border-radius: 50%;\n",
    "                animation: bounce 1.4s infinite;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(2) {\n",
    "                animation-delay: 0.2s;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(3) {\n",
    "                animation-delay: 0.4s;\n",
    "            }\n",
    "\n",
    "            @keyframes fadeIn {\n",
    "                from {\n",
    "                    opacity: 0;\n",
    "                    transform: translateY(10px);\n",
    "                }\n",
    "                to {\n",
    "                    opacity: 1;\n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            @keyframes bounce {\n",
    "                0%, 80%, 100% { \n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "                40% {\n",
    "                    transform: translateY(-8px);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            .input-container {\n",
    "                background: white;\n",
    "                padding: 20px;\n",
    "                box-shadow: 0 -2px 10px rgba(0,0,0,0.05);\n",
    "            }\n",
    "\n",
    "            .input-wrapper {\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                display: flex;\n",
    "                gap: 10px;\n",
    "            }\n",
    "\n",
    "            textarea {\n",
    "                flex: 1;\n",
    "                padding: 12px 16px;\n",
    "                border: 2px solid #e2e8f0;\n",
    "                border-radius: 12px;\n",
    "                resize: none;\n",
    "                font-size: 16px;\n",
    "                transition: border-color 0.2s;\n",
    "            }\n",
    "\n",
    "            textarea:focus {\n",
    "                outline: none;\n",
    "                border-color: var(--primary-color);\n",
    "            }\n",
    "\n",
    "            button {\n",
    "                background: var(--primary-color);\n",
    "                color: white;\n",
    "                border: none;\n",
    "                padding: 12px 24px;\n",
    "                border-radius: 12px;\n",
    "                cursor: pointer;\n",
    "                font-weight: 600;\n",
    "                transition: transform 0.2s, background 0.2s;\n",
    "            }\n",
    "\n",
    "            button:hover {\n",
    "                background: #1d4ed8;\n",
    "                transform: translateY(-1px);\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"chat-container\" id=\"chatContainer\">\n",
    "            <div class=\"message bot-message\">\n",
    "                <div class=\"typing-indicator\">\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-container\">\n",
    "            <div class=\"input-wrapper\">\n",
    "                <textarea id=\"prompt\" placeholder=\"Mesajınızı yazın...\" rows=\"1\"></textarea>\n",
    "                <button onclick=\"generate()\">Gönder</button>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            const chatContainer = document.getElementById('chatContainer');\n",
    "            const textarea = document.getElementById('prompt');\n",
    "\n",
    "            // Otomatik textarea yüksekliği ayarı\n",
    "            textarea.addEventListener('input', () => {\n",
    "                textarea.style.height = 'auto';\n",
    "                textarea.style.height = textarea.scrollHeight + 'px';\n",
    "            });\n",
    "\n",
    "            function addMessage(text, isUser) {\n",
    "                const messageDiv = document.createElement('div');\n",
    "                messageDiv.className = `message ${isUser ? 'user-message' : 'bot-message'}`;\n",
    "                \n",
    "                if(!isUser) {\n",
    "                    const typingIndicator = document.querySelector('.typing-indicator');\n",
    "                    if(typingIndicator) typingIndicator.remove();\n",
    "                }\n",
    "                \n",
    "                messageDiv.innerHTML = text.replace(/\\n/g, '<br>');\n",
    "                chatContainer.appendChild(messageDiv);\n",
    "                messageDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "            }\n",
    "\n",
    "            function generate() {\n",
    "                const prompt = textarea.value.trim();\n",
    "                if(!prompt) return;\n",
    "\n",
    "                // Önceki bağlantıyı kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                // Kullanıcı mesajını ekle\n",
    "                addMessage(prompt, true);\n",
    "                textarea.value = '';\n",
    "                \n",
    "                // Typing indicator ekle\n",
    "                const typingDiv = document.createElement('div');\n",
    "                typingDiv.className = 'message bot-message';\n",
    "                typingDiv.innerHTML = `\n",
    "                    <div class=\"typing-indicator\">\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                    </div>\n",
    "                `;\n",
    "                chatContainer.appendChild(typingDiv);\n",
    "                typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "\n",
    "                // Yeni bağlantı oluştur\n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                let botResponse = '';\n",
    "                currentEventSource.onmessage = (e) => {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        botResponse += e.data;\n",
    "                        typingDiv.innerHTML = botResponse.replace(/\\n/g, '<br>');\n",
    "                        typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // Enter tuşu desteği\n",
    "            textarea.addEventListener('keydown', (e) => {\n",
    "                if(e.key === 'Enter' && !e.shiftKey) {\n",
    "                    e.preventDefault();\n",
    "                    generate();\n",
    "                }\n",
    "            });\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Önceki işlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
