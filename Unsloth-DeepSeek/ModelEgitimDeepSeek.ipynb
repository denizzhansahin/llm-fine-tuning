{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth_zoo in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2025.2.7)\n",
      "Requirement already satisfied: torch in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (2.6.0+cu124)\n",
      "Requirement already satisfied: packaging in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (24.1)\n",
      "Requirement already satisfied: tyro in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.9.16)\n",
      "Collecting transformers>=4.46.1 (from unsloth_zoo)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (1.26.4)\n",
      "Collecting accelerate>=0.34.1 (from unsloth_zoo)\n",
      "  Using cached accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.15.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.14.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.29.1)\n",
      "Requirement already satisfied: hf_transfer in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (0.1.9)\n",
      "Requirement already satisfied: cut_cross_entropy in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (25.1.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from unsloth_zoo) (10.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate>=0.34.1->unsloth_zoo) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from datasets>=2.16.0->unsloth_zoo) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->unsloth_zoo) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->unsloth_zoo) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers>=4.46.1->unsloth_zoo) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.46.1->unsloth_zoo)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tyro->unsloth_zoo) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch->unsloth_zoo) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.17.0)\n",
      "Using cached accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.32.1\n",
      "    Uninstalling accelerate-0.32.1:\n",
      "      Successfully uninstalled accelerate-0.32.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.4\n",
      "    Uninstalling transformers-4.42.4:\n",
      "      Successfully uninstalled transformers-4.42.4\n",
      "Successfully installed accelerate-1.4.0 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"senin token\"  # Hugging Face Ã¼zerinden ince ayar iÃ§in kullanacaÄŸÄ±mÄ±z modeli indirmek iÃ§in Hugging Face tokenÄ± gerekmektedir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:24<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\Desktop\\Denizhan2\\deepseek-14B does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneyi iÃ§e aktarma\n",
    "from unsloth import FastLanguageModel  # Unsloth kÃ¼tÃ¼phanesinden hÄ±zlÄ± model yÃ¼kleme sÄ±nÄ±fÄ±nÄ± iÃ§e aktarÄ±r, bÃ¼yÃ¼k dil modellerini optimize eder\n",
    "\n",
    "# PyTorch kÃ¼tÃ¼phanesini iÃ§e aktarma\n",
    "import torch                           # PyTorch'u tensÃ¶r iÅŸlemleri ve model hesaplamalarÄ± iÃ§in iÃ§e aktarÄ±r\n",
    "\n",
    "# Maksimum dizi uzunluÄŸu ayarÄ±\n",
    "max_seq_length = 2048                  # Modelin bir seferde iÅŸleyebileceÄŸi maksimum token sayÄ±sÄ±nÄ± 2048 olarak tanÄ±mlar, RoPE Scaling ile otomatik Ã¶lÃ§eklenir\n",
    "\n",
    "# Veri tipi ayarÄ±\n",
    "#dtype = None                           # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r; Tesla T4/V100 iÃ§in Float16, Ampere+ iÃ§in Bfloat16 seÃ§ilir\n",
    "dtype = torch.bfloat16 #A5000 ekran kartÄ± iÃ§in bunu seÃ§tik.\n",
    "\n",
    "# 4-bit kuantizasyon ayarÄ±\n",
    "load_in_4bit = True                    # Modeli 4-bit kuantizasyon ile yÃ¼kler, bu bellek kullanÄ±mÄ±nÄ± azaltÄ±r ve performansÄ± korur (False yapÄ±lÄ±rsa tam hassasiyet kullanÄ±lÄ±r)\n",
    "\n",
    "# 4-bit Ã¶nceden kuantize edilmiÅŸ modellerin listesi\n",
    "fourbit_models = [                     # Unslothâ€™un desteklediÄŸi, 4-bit kuantize edilmiÅŸ modellerin listesi; hÄ±zlÄ± indirme ve bellek tasarrufu saÄŸlar\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 8B modeli, 15 trilyon token ile eÄŸitilmiÅŸ, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",  # Llama-3.1 8Bâ€™nin talimatlara Ã¶zel sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",     # Llama-3.1 70B modeli\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # Llama-3.1 405B modeli, 4-bit olarak yÃ¼kleniyor\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # Mistral Nemo temel modeli, 12B, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",  # Mistral Nemoâ€™nun talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral 7B v0.3, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",   # Mistral 7B v0.3 talimat sÃ¼rÃ¼mÃ¼\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 mini, talimatlara Ã¶zel, 2 kat hÄ±zlÄ±\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",        # Phi-3 orta boy, 4k token kapasiteli\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",             # Gemma 2 9B modeli\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2 27B modeli, 2 kat hÄ±zlÄ±\n",
    "]  # Daha fazla model iÃ§in: https://huggingface.co/unsloth\n",
    "model_path = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\deepseek-14B\"\n",
    "\n",
    "# Modeli ve tokenizer'Ä± Ã¶nceden eÄŸitilmiÅŸ olarak yÃ¼kleme\n",
    "model, tokenizer = FastLanguageModel.from_pretrained( \n",
    "   # force_download=True,\n",
    "      # Modeli ve tokenizer'Ä± yÃ¼kler, Unsloth optimizasyonlarÄ±yla\n",
    "    model_name=model_path,   # YÃ¼klenecek modelin adÄ±: LLaMA-3.1 8B (8 milyar parametre)\n",
    "    max_seq_length=max_seq_length,            # Maksimum dizi uzunluÄŸunu 2048 olarak ayarlar\n",
    "    dtype=dtype,                              # Veri tipini otomatik algÄ±lamaya bÄ±rakÄ±r (Tesla T4 iÃ§in Float16 olur)\n",
    "    load_in_4bit=load_in_4bit,                # 4-bit kuantizasyonu etkinleÅŸtirir, belleÄŸi optimize eder\n",
    "    #token = \"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\",                       # EÄŸer kÄ±sÄ±tlÄ± eriÅŸimli bir model kullanÄ±yorsanÄ±z Hugging Face tokenâ€™Ä± eklenir (burada pasif)\n",
    ")  # Model ve tokenizer nesnelerini dÃ¶ndÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR =  \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\" #Model verilerini kayÄ±t edeceÄŸiniz klasÃ¶rÃ¼ seÃ§in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Modeli LoRA ile yapÄ±landÄ±rma\n",
    "model = FastLanguageModel.get_peft_model(  # Modeli LoRA ile optimize edilmiÅŸ bir PEFT (Parameter-Efficient Fine-Tuning) modeline dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r\n",
    "    model,                                 # LoRA'nÄ±n uygulanacaÄŸÄ± ana model (Ã¶nceki adÄ±mda yÃ¼klenen model)\n",
    "    r=32,                                  # LoRA'nÄ±n rank deÄŸeri: 16â€™dan 32â€™ye artÄ±rÄ±ldÄ± (RTX A5000â€™in 24 GB VRAMâ€™i daha yÃ¼ksek rank iÃ§in yeterli, daha iyi performans iÃ§in) eskisi ise 16\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],  # LoRA'nÄ±n uygulanacaÄŸÄ± Transformer katmanlarÄ± (dikkat mekanizmasÄ± ve feed-forward katmanlarÄ±, deÄŸiÅŸmedi)\n",
    "    lora_alpha=32,                         # LoRA'nÄ±n Ã¶lÃ§eklendirme faktÃ¶rÃ¼: 16â€™dan 32â€™ye artÄ±rÄ±ldÄ± (r ile uyumlu, daha gÃ¼Ã§lÃ¼ Ã¶ÄŸrenme iÃ§in) eskisi ise 16\n",
    "    lora_dropout=0.05,                     # LoRA iÃ§in dropout oranÄ±: 0â€™dan 0.05â€™e artÄ±rÄ±ldÄ± (bÃ¼yÃ¼k veri setinde overfitting riskine karÅŸÄ±) eskisi ise 0\n",
    "    bias=\"none\",                           # Bias ayarÄ±: \"none\" (deÄŸiÅŸmedi, optimize edilmiÅŸ bir seÃ§enek)\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Gradient checkpointing ayarÄ±: \"unsloth\" (deÄŸiÅŸmedi, VRAMâ€™i %30 azaltÄ±r, 2 kat bÃ¼yÃ¼k batch size saÄŸlar)\n",
    "    random_state=3407,                     # Rastgele durum tohumu: 3407 (deÄŸiÅŸmedi, tekrarlanabilirlik iÃ§in sabit)\n",
    "    use_rslora=True,                       # Rank Stabilized LoRA (RSLoRA) kullanÄ±mÄ±: Falseâ€™tan Trueâ€™ya deÄŸiÅŸtirildi (stabiliteyi artÄ±rÄ±r, bÃ¼yÃ¼k veri setlerinde faydalÄ±) eskisi ise False\n",
    "    loftq_config=None,                     # LoftQ (Low-Rank Factorized Quantization) yapÄ±landÄ±rmasÄ±: None (deÄŸiÅŸmedi, ihtiyaÃ§ yoksa ek optimizasyon gerekmez)\n",
    ")                                          # LoRA ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ yeni modeli dÃ¶ndÃ¼rÃ¼r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ± iÃ§in prompt ÅŸablonu tanÄ±mlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # ÃœÃ§ bÃ¶lÃ¼mden oluÅŸan bir ÅŸablon: talimat (instruction), giriÅŸ (input) ve cevap (response) yer tutucularÄ± iÃ§erir\n",
    "\n",
    "# EOS_TOKEN tanÄ±mlama\n",
    "EOS_TOKEN = tokenizer.eos_token  # Tokenizer'dan EOS (End of Sequence) token'Ä±nÄ± alÄ±r, metnin sonunu iÅŸaretler\n",
    "\n",
    "# Veri setini biÃ§imlendirme fonksiyonu\n",
    "def formatting_prompts_func(examples):  # Veri setindeki Ã¶rnekleri Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in bir fonksiyon\n",
    "    instructions = examples[\"instruction\"]  # Veri setindeki \"instruction\" sÃ¼tununu alÄ±r (talimatlar)\n",
    "    inputs       = examples[\"input\"]        # Veri setindeki \"input\" sÃ¼tununu alÄ±r (giriÅŸler)\n",
    "    outputs      = examples[\"output\"]       # Veri setindeki \"output\" sÃ¼tununu alÄ±r (cevaplar)\n",
    "    texts = []                              # BiÃ§imlendirilmiÅŸ metinleri saklamak iÃ§in boÅŸ bir liste oluÅŸturur\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):  # Her bir talimat, giriÅŸ ve cevap Ã¼Ã§lÃ¼sÃ¼nÃ¼ eÅŸleÅŸtirir\n",
    "        # Alpaca ÅŸablonunu doldurur ve EOS_TOKEN ekler, yoksa Ã¼retim sonsuza dek devam eder!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN  # Åžablonu doldurur ve metnin sonuna EOS tokenâ€™Ä± ekler\n",
    "        texts.append(text)                  # BiÃ§imlendirilmiÅŸ metni listeye ekler\n",
    "    return { \"text\" : texts, }              # \"text\" anahtarÄ±yla biÃ§imlendirilmiÅŸ metinleri bir sÃ¶zlÃ¼k olarak dÃ¶ndÃ¼rÃ¼r\n",
    "pass                                       # BoÅŸ bir \"pass\" ifadesi (gereksiz, fonksiyon zaten tamamlandÄ±)\n",
    "\n",
    "# Veri setini yÃ¼kleme ve iÅŸleme\n",
    "from datasets import load_dataset     \n",
    "from datasets import load_from_disk\n",
    "  # Hugging Faceâ€™in datasets kÃ¼tÃ¼phanesini iÃ§e aktarÄ±r\n",
    "dataset = load_dataset(\"json\", data_files=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan\\\\alpaca_format.json\")[\"train\"]  # JSON dosyasÄ±ndan veri setini yÃ¼kler ve \"train\" bÃ¶lÃ¼mÃ¼nÃ¼ alÄ±r\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)  # Veri setine biÃ§imlendirme fonksiyonunu toplu (batched) ÅŸekilde uygular\n",
    "\n",
    "#tokenized_dataset = load_from_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset\")\n",
    "\n",
    "# EÄŸitim iÃ§in dataset'i ayarla\n",
    "#dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def add_attention_mask(example):\n",
    "    example[\"attention_mask\"] = [1] * len(example[\"input_ids\"])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_attention_mask, num_proc=4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4763907/4763907 [48:03<00:00, 1652.25 examples/s]\n",
      "Packing train dataset (num_proc=4): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4763907/4763907 [1:51:45<00:00, 710.45 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarma\n",
    "from trl import SFTTrainer              # TRL kÃ¼tÃ¼phanesinden SFTTrainerâ€™Ä± iÃ§e aktarÄ±r, denetimli ince ayar iÃ§in kullanÄ±lÄ±r\n",
    "from transformers import TrainingArguments  # Hugging Face Transformersâ€™tan eÄŸitim argÃ¼manlarÄ±nÄ± iÃ§e aktarÄ±r\n",
    "from unsloth import is_bfloat16_supported   # Unslothâ€™tan bfloat16 desteÄŸini kontrol eden bir fonksiyon alÄ±r\n",
    "\n",
    "# EÄŸitim sÃ¼recini baÅŸlatmak iÃ§in trainer nesnesi oluÅŸturma\n",
    "trainer = SFTTrainer(                   # Modeli eÄŸitmek iÃ§in SFTTrainer nesnesi oluÅŸturur\n",
    "    model=model,                        # EÄŸitilecek model (Ã¶nceki adÄ±mlarda LoRA ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ)\n",
    "    tokenizer=tokenizer,                # Modelin tokenizerâ€™Ä± (metni tokenâ€™lara Ã§evirir)\n",
    "    train_dataset=dataset,              # EÄŸitim iÃ§in kullanÄ±lacak veri seti (Alpaca formatÄ±nda iÅŸlenmiÅŸ)\n",
    "    dataset_text_field=\"text\",          # Veri setindeki hangi sÃ¼tunun kullanÄ±lacaÄŸÄ±: \"text\" (biÃ§imlendirilmiÅŸ promptâ€™lar)\n",
    "    max_seq_length=2048,                # Maksimum dizi uzunluÄŸu (Ã¶nceki adÄ±mda 2048 olarak tanÄ±mlÄ±, uzun baÄŸlamlar iÃ§in yeterli)\n",
    "    dataset_num_proc=4,                 # Veri iÅŸleme iÃ§in kullanÄ±lacak iÅŸlemci sayÄ±sÄ±: 4 (RTX A5000â€™in gÃ¼cÃ¼yle paralel iÅŸleme artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in daha hÄ±zlÄ±)\n",
    "    packing=True,                       # Veri paketlemeyi etkinleÅŸtirir; kÄ±sa diziler iÃ§in 5x hÄ±z artÄ±ÅŸÄ± saÄŸlar, bÃ¼yÃ¼k veri setinde faydalÄ±\n",
    "    args=TrainingArguments(             # EÄŸitim argÃ¼manlarÄ±nÄ± tanÄ±mlar\n",
    "        per_device_train_batch_size=8,  # Her cihaz (GPU) iÃ§in eÄŸitim batch boyutu: 8 (24 GB VRAM ile mÃ¼mkÃ¼n, performans artÄ±rÄ±ldÄ±)\n",
    "        gradient_accumulation_steps=4,  # Gradyan biriktirme adÄ±mlarÄ±: 4 (efektif batch sizeâ€™Ä± 8*4=32 yapar, bellek yÃ¶netimi iÃ§in)\n",
    "        warmup_steps=100,               # Ã–ÄŸrenme oranÄ± Ä±sÄ±nma adÄ±mlarÄ±: 100 (bÃ¼yÃ¼k veri seti iÃ§in daha yavaÅŸ ve stabil Ä±sÄ±nma, 5â€™ten artÄ±rÄ±ldÄ±)\n",
    "        num_train_epochs=2,           # Tam bir epoch iÃ§in eÄŸitim (yorum satÄ±rÄ±, ÅŸu anda kullanÄ±lmÄ±yor, veri seti bÃ¼yÃ¼k olduÄŸu iÃ§in adÄ±m bazlÄ± tercih ediliyor)\n",
    "        max_steps=1500,                 # Toplam eÄŸitim adÄ±mÄ± sayÄ±sÄ±: 1000 (60â€™tan artÄ±rÄ±ldÄ±, 5 milyon satÄ±r iÃ§in yeterli Ã¶ÄŸrenme saÄŸlamak iÃ§in)\n",
    "        learning_rate=2e-4,             # Ã–ÄŸrenme oranÄ±: 0.0002 (modelin ne kadar hÄ±zlÄ± Ã¶ÄŸreneceÄŸini belirler, bÃ¼yÃ¼k veri setinde uygun)\n",
    "        fp16=not is_bfloat16_supported(),  # Float16 kullanÄ±mÄ±: bfloat16 desteklenmiyorsa True (RTX A5000â€™de bfloat16 destekleniyor)\n",
    "        bf16=is_bfloat16_supported(),   # Bfloat16 kullanÄ±mÄ±: destekleniyorsa True (Ampere mimarisi iÃ§in RTX A5000â€™de etkin, daha verimli)\n",
    "        logging_steps=10,               # Log kayÄ±t sÄ±klÄ±ÄŸÄ±: her 10 adÄ±mda bir (1â€™den artÄ±rÄ±ldÄ±, bÃ¼yÃ¼k veri setinde log sÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r)\n",
    "        optim=\"adamw_8bit\",             # Optimize edici: 8-bit AdamW (bellek verimli bir versiyon, deÄŸiÅŸmedi)\n",
    "        weight_decay=0.01,              # AÄŸÄ±rlÄ±k Ã§Ã¼rÃ¼mesi: 0.01 (overfittingâ€™i Ã¶nlemek iÃ§in regularization, deÄŸiÅŸmedi)\n",
    "        lr_scheduler_type=\"linear\",     # Ã–ÄŸrenme oranÄ± zamanlayÄ±cÄ±sÄ±: doÄŸrusal (lineer bir ÅŸekilde azalÄ±r, deÄŸiÅŸmedi)\n",
    "        seed=3407,                      # Rastgele tohum: 3407 (tekrarlanabilirlik iÃ§in, deÄŸiÅŸmedi)\n",
    "        output_dir=CHECKPOINT_DIR,      # Ã‡Ä±ktÄ±larÄ±n kaydedileceÄŸi dizin: \"CHECKPOINT_DIR\" (deÄŸiÅŸmedi)\n",
    "        report_to=\"none\",               # EÄŸitim raporlama: \"none\" (WandB gibi araÃ§lar kullanÄ±lmayacak, deÄŸiÅŸmedi)\n",
    "    ),                                  # EÄŸitim argÃ¼manlarÄ±nÄ± tamamlar\n",
    ")                                       # Trainer nesnesini oluÅŸturur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (42/42 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2518276/2518276 [00:12<00:00, 201447.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenâ€™lanmÄ±ÅŸ veri setini kaydet\n",
    "trainer.train_dataset.save_to_disk(f\"{CHECKPOINT_DIR}/tokenized_dataset2_atten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint aranÄ±yor...\n",
      "Hata: No valid checkpoint found in output directory (C:\\Users\\Mehmet\\Desktop\\Denizhan2\\model_egitim)...\n",
      "Yeni eÄŸitim baÅŸlatÄ±lÄ±yor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,518,276 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 32 | Total steps = 1,500\n",
      " \"-____-\"     Number of trainable parameters = 137,625,600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 39:49:13, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.231500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.086700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.994100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.933500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.858600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.871200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.823500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.792300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.759800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.719700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.750100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.774900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.720600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.747000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.743100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.730300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.674600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.625700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.634000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.652800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.647500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.637900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.649500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.597500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.622300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.619000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.609800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.577100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.602500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.630500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# 9. EÄŸitimi BaÅŸlat\n",
    "# ----------------------------------------------\n",
    "try:                                    # Hata yakalama bloÄŸu baÅŸlatÄ±r, checkpoint ile devam etmeyi dener\n",
    "    print(\"Checkpoint aranÄ±yor...\")     # KullanÄ±cÄ±ya checkpointâ€™in arandÄ±ÄŸÄ±nÄ± bildirir\n",
    "    trainer.train(resume_from_checkpoint=True)  # EÄŸitime son checkpointâ€™ten devam etmeyi dener (Ã¶nceki bir eÄŸitim varsa)\n",
    "except Exception as e:                  # EÄŸer bir hata oluÅŸursa (Ã¶rneÄŸin, checkpoint bulunamazsa) bu blok Ã§alÄ±ÅŸÄ±r\n",
    "    print(f\"Hata: {str(e)[:200]}...\\nYeni eÄŸitim baÅŸlatÄ±lÄ±yor...\")  # HatanÄ±n ilk 200 karakterini yazdÄ±rÄ±r ve yeni eÄŸitime geÃ§eceÄŸini bildirir\n",
    "    trainer.train()                     # SÄ±fÄ±rdan yeni bir eÄŸitim baÅŸlatÄ±r (checkpoint olmadan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model eÄŸitimini durdurmak gerekiyordu Ã§Ã¼nkÃ¼ 26 saat istedi.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 step iÃ§in 26 saat verdi ve 137,625,600 parametre eÄŸitilecek. Bunu hafta sonu 2000 step iÃ§in 52 saat+3 saat tokenizer olarak yaptÄ±ÄŸÄ±mÄ±zda 274 milyon parametre eÄŸitilecek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:14<00:00,  1.21s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca formatÄ± iÃ§in prompt ÅŸablonu tanÄ±mlama\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"  # ÃœÃ§ bÃ¶lÃ¼mden oluÅŸan bir ÅŸablon: talimat (instruction), giriÅŸ (input) ve cevap (response) yer tutucularÄ± iÃ§erir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "FenerbahÃ§e'nin son maÃ§larÄ±nÄ±n sonuÃ§larÄ±nÄ± listele ve her maÃ§ iÃ§in kÄ±sa bir aÃ§Ä±klama yap.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "FenerbahÃ§e'nin son maÃ§larÄ± ÅŸunlardÄ±r:\n",
      "\n",
      " 19 EylÃ¼l 2014 - Ä°stanbul - FenerbahÃ§e - Galatasaray - 3-1 - SÃ¼per Kupa\n",
      " 19 EylÃ¼l 2014 - Ä°stanbul - FenerbahÃ§e - Adana Demirspor - 2-1 - SÃ¼per Lig\n",
      " 27 EylÃ¼l 2014 - Ä°stanbul - FenerbahÃ§e - Gaziantepspor - 2-0 - SÃ¼per Lig\n",
      " 11 Ekim 2014 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 1-0 - SÃ¼per Lig\n",
      " 15 Ekim 2014 - Ä°stanbul - FenerbahÃ§e - Adana Demirspor - 3-1 - SÃ¼per Lig\n",
      " 19 Ekim 2014 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 3-0 - SÃ¼per Lig\n",
      " 26 Ekim 2014 - Ä°stanbul - FenerbahÃ§e - Sivasspor - 1-0 - SÃ¼per Lig\n",
      " 2 KasÄ±m 2014 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 10 KasÄ±m 2014 - Ä°stanbul - FenerbahÃ§e - Galatasaray - 2-1 - SÃ¼per Lig\n",
      " 23 KasÄ±m 2014 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 3-1 - SÃ¼per Lig\n",
      " 26 KasÄ±m 2014 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 8 AralÄ±k 2014 - Ä°stanbul - FenerbahÃ§e - Adana Demirspor - 2-0 - SÃ¼per Lig\n",
      " 13 AralÄ±k 2014 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 2-0 - SÃ¼per Lig\n",
      " 20 AralÄ±k 2014 - Ä°stanbul - FenerbahÃ§e - Sivasspor - 2-1 - SÃ¼per Lig\n",
      " 27 AralÄ±k 2014 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 1-0 - SÃ¼per Lig\n",
      " 1 Ocak 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 1-1 - SÃ¼per Lig\n",
      " 4 Ocak 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 10 Ocak 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 2-0 - SÃ¼per Lig\n",
      " 25 Ocak 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 28 Ocak 2015 - Ä°stanbul - FenerbahÃ§e - Sivasspor - 2-0 - SÃ¼per Lig\n",
      " 11 Åžubat 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 3-1 - SÃ¼per Lig\n",
      " 14 Åžubat 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 3-0 - SÃ¼per Lig\n",
      " 21 Åžubat 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 2-0 - SÃ¼per Lig\n",
      " 28 Åžubat 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 12 Mart 2015 - Ä°stanbul - FenerbahÃ§e - Sivasspor - 1-1 - SÃ¼per Lig\n",
      " 16 Mart 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 1-1 - SÃ¼per Lig\n",
      " 22 Mart 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 2-1 - SÃ¼per Lig\n",
      " 26 Mart 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 3-1 - SÃ¼per Lig\n",
      " 29 Mart 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 3-0 - SÃ¼per Lig\n",
      " 12 Nisan 2015 - Ä°stanbul - FenerbahÃ§e - KasÄ±mpaÅŸa - 1-0 - SÃ¼per Lig\n",
      " 15 Nisan 2015 - Ä°stanbul - FenerbahÃ§e - Trabzonspor - 3-2 - SÃ¼per Lig\n",
      " 22 Nisan 2015 - Ä°stanbul - F\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "        alpaca_prompt.format(           # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"FenerbahÃ§e'nin son maÃ§larÄ±nÄ±n sonuÃ§larÄ±nÄ± listele ve her maÃ§ iÃ§in kÄ±sa bir aÃ§Ä±klama yap.\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "            \"\",                         # GiriÅŸ: BoÅŸ bÄ±rakÄ±ldÄ±, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=1024,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "TÃ¼rkiye SÃ¼per Lig futbol takÄ±mlarÄ± hakÄ±nda bilgi ver.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "TÃ¼rkiye SÃ¼per Lig, 1959'da kurulan TÃ¼rk futbol ligi. TÃ¼rkiye'de oynanan profesyonel futbol ligidir. TÃ¼rkiye'de 1959-60 sezonunda baÅŸlamÄ±ÅŸtÄ±r. SÃ¼per Lig, TÃ¼rkiye Futbol Federasyonu tarafÄ±ndan dÃ¼zenlenir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en Ã¼st dÃ¼zey ligdir. SÃ¼per Lig, TÃ¼rkiye'de oynanan en\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "        alpaca_prompt.format(           # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"TÃ¼rkiye SÃ¼per Lig futbol takÄ±mlarÄ± hakÄ±nda bilgi ver.\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "            \"\",                         # GiriÅŸ: BoÅŸ bÄ±rakÄ±ldÄ±, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=1024,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denizhan Åžahin kimdir? Denizhan Åžahin kaÃ§ yaÅŸÄ±nda? Denizhan Åžahin nereli?\n",
      "Denizhan Åžahin kimdir? Denizhan Åžahin kaÃ§ yaÅŸÄ±nda? Denizhan Åžahin nereli? Denizhan Åžahin biyografisi, Denizhan Åžahin hayatÄ± hakkÄ±nda bilgiler...\n",
      "Denizhan Åžahin kimdir?\n",
      "Denizhan Åžahin (d. 1992, Ä°stanbul), TÃ¼rk oyuncu. 2015 yÄ±lÄ±nda ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Ã‡iÄŸdem\" karakteri ile yaÅŸayan Åžahin, \"GÃ¶nÃ¼l YaralanÄ±ÅŸÄ±\", \"KardeÅŸler\", \"Kara Korsanlar\", \"ÅžimÅŸekler AteÅŸe KarÅŸÄ±\", \"Sevgili Evren\", \"Kara AÄŸa\", \"KÄ±zÄ±l Yol\", \"Ã‡ocuklar\" ve \"Ä°stanbul'un OÄŸlu\" gibi dizilerde rol aldÄ±.\n",
      "Denizhan Åžahin kaÃ§ yaÅŸÄ±nda?\n",
      "Denizhan Åžahin 1992 yÄ±lÄ±nda doÄŸduÄŸu iÃ§in 2022 yÄ±lÄ±nda 30 yaÅŸÄ±nda.\n",
      "Denizhan Åžahin nereli?\n",
      "Denizhan Åžahin Ä°stanbul'da doÄŸdu.\n",
      "Denizhan Åžahin biyografisi\n",
      "Denizhan Åžahin, 1992 yÄ±lÄ±nda Ä°stanbul'da doÄŸdu. Ä°lk ve orta Ã¶ÄŸrenimini Ä°stanbul'da tamamladÄ±. Liseyi Ä°stanbul'da tamamladÄ±ktan sonra Ä°stanbul Ãœniversitesi, Ä°letiÅŸim FakÃ¼ltesi'nde eÄŸitim gÃ¶rdÃ¼. 2015 yÄ±lÄ±nda ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Ã‡iÄŸdem\" karakteri ile yaÅŸayan Åžahin, \"GÃ¶nÃ¼l YaralanÄ±ÅŸÄ±\", \"KardeÅŸler\", \"Kara Korsanlar\", \"ÅžimÅŸekler AteÅŸe KarÅŸÄ±\", \"Sevgili Evren\", \"Kara AÄŸa\", \"KÄ±zÄ±l Yol\", \"Ã‡ocuklar\" ve \"Ä°stanbul'un OÄŸlu\" gibi dizilerde rol aldÄ±.\n",
      "Denizhan Åžahin hayatÄ± hakkÄ±nda bilgiler\n",
      "Denizhan Åžahin, 1992 yÄ±lÄ±nda Ä°stanbul'da doÄŸdu. Ä°lk ve orta Ã¶ÄŸrenimini Ä°stanbul'da tamamladÄ±. Liseyi Ä°stanbul'da tamamladÄ±ktan sonra Ä°stanbul Ãœniversitesi, Ä°letiÅŸim FakÃ¼ltesi'nde eÄŸitim gÃ¶rdÃ¼. 2015 yÄ±lÄ±nda ilk oyunculuk deneyimini \"Bir Yol Bir Hikaye\" dizisindeki \"Ã‡iÄŸdem\" karakteri ile yaÅŸayan Åžahin, \"GÃ¶nÃ¼l YaralanÄ±ÅŸÄ±\", \"KardeÅŸler\", \"Kara Korsanlar\", \"ÅžimÅŸekler AteÅŸe KarÅŸÄ±\", \"Sevgili Evren\", \"Kara AÄŸa\", \"KÄ±zÄ±l Yol\", \"Ã‡ocuklar\" ve \"Ä°stanbul'un OÄŸlu\" gibi dizilerde rol aldÄ±.\n",
      "Denizhan Åžahin, \"Kara Korsanlar\", \"Kara AÄŸa\", \"Ä°stanbul'un OÄŸlu\" ve \"Ã‡ocuklar\" dizilerindeki performansÄ± ile \"En Ä°yi KadÄ±n Oyuncu\" dalÄ±nda AltÄ±n Kelebek Ã–dÃ¼lleri'ne aday gÃ¶sterildi.\n",
      "Denizhan Åžahin, 2017 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2018 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2019 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2020 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2021 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2022 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2023 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2024 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2025 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2026 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2027 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2028 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2029 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2030 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2031 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2032 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2033 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2034 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2035 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2036 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2037 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2038 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2039 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2040 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2041 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2042 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2043 yÄ±lÄ±nda \"Kara Korsanlar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼lÃ¼nÃ¼ aldÄ±.\n",
      "Denizhan Åžahin, 2044 yÄ±lÄ±nda \"Ã‡ocuklar\" dizisiyle AltÄ±n Kelebek Ã–dÃ¼lleri'nde \"En Ä°yi KadÄ±n Oyuncu\" Ã¶dÃ¼l\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "               # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"Denizhan Åžahin kimdir?\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "                                   # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=2048,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sen hangi yapay zeka modelisin? Kendini tanÄ±t.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "               # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"Sen hangi yapay zeka modelisin? Kendini tanÄ±t.\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "                                   # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=2048,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek nedir? DeepSeek, yapay zeka ve bÃ¼yÃ¼k veri teknolojilerini kullanarak bilgi ve bilgi kaynaÄŸÄ± arayÄ±cÄ±sÄ± olarak Ã§alÄ±ÅŸan bir teknoloji ÅŸirketidir..DeepSeek, bÃ¼yÃ¼k veri teknolojilerini ve yapay zeka algoritmalarÄ±nÄ± kullanarak bilgi ve bilgi kaynaÄŸÄ± arayÄ±cÄ±sÄ± olarak Ã§alÄ±ÅŸan bir teknoloji ÅŸirketidir. DeepSeek, verileri analiz ederek, bilgi aramasÄ± ve analizlerde daha doÄŸru ve verimli sonuÃ§lar elde etmenizi saÄŸlar. AyrÄ±ca, ÅŸirket, kullanÄ±cÄ±larÄ±n ihtiyaÃ§larÄ±na gÃ¶re Ã¶zelleÅŸtirilmiÅŸ veri analizi ve raporlama hizmetleri de sunar. DeepSeek, Ã¶zellikle finans, saÄŸlÄ±k, pazarlama ve gÃ¼venlik gibi alanlarda kullanÄ±lmaktadÄ±r.\n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "               # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"DeepSeek nedir?\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "                                   # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        \n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=2048,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Ã‡Ä±ktÄ±yÄ± decode etme ve yazdÄ±rma\n",
    "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]  # Ãœretilen tokenâ€™larÄ± insan tarafÄ±ndan okunabilir metne Ã§evirir, Ã¶zel tokenâ€™larÄ± atlar\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kÄ±sÄ±m\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Streamer'Ä± oluÅŸtur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # BaÅŸlangÄ±Ã§ promptunu gÃ¶sterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [\"Denizhan Åžahin kimdir?\"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate iÃ§inde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'Ä± parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 puanlÄ± bir takÄ±m. Benim iÃ§in 100 puanlÄ± bir takÄ±m. Bu takÄ±mÄ±n Ã¶nÃ¼nde bir ÅŸey yok. Bu takÄ±mÄ±n Ã¶nÃ¼nde hiÃ§bir ÅŸey yok. Benim iÃ§in 100 puanlÄ± bir takÄ±m. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oynuyor. Kendi oyununu oyn\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"FenerbahÃ§e nasÄ±l bir takÄ±m? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate iÃ§inde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'Ä± parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.06.2018, 15:41 10.06.2018, 15:41\n",
      "Teknoloji Festivalleri ve YarÄ±ÅŸmalarÄ± (Teknofest) kapsamÄ±nda dÃ¼zenlenen 1. UÃ§ak MÃ¼hendisliÄŸi YarÄ±ÅŸmasÄ±nda birinci olan Ã‡ankaya Ãœniversitesine ait ekip, 10 AÄŸustos 2018 tarihinde yapÄ±lacak Teknofest Ä°stanbul'da TÃ¼rkiye'yi temsil edecek. Teknofest Ä°stanbul 2018'te 10 AÄŸustos 2018 tarihinde baÅŸlayacak. 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul 2018'de, 10 AÄŸustos'ta baÅŸlayacak Teknofest Ä°stanbul "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      2\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeknofest iÃ§in bilgi verebilir misin? \u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      3\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate iÃ§inde streamer kullan\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Streamer'Ä± parametre olarak ekle\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1596\u001b[0m, in \u001b[0;36m_wrap_fast_inference.<locals>._fast_generate\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;66;03m# Set pad token\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;66;03m# old_pad_token_id = getattr(model.config, \"pad_token_id\", None)\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;66;03m# old_eos_token_id = getattr(model.config, \"eos_token_id\", None)\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_eos_token_id\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \n\u001b[0;32m   1594\u001b[0m \u001b[38;5;66;03m# Autocasted\u001b[39;00m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type \u001b[38;5;241m=\u001b[39m device_type, dtype \u001b[38;5;241m=\u001b[39m dtype):\n\u001b[1;32m-> 1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;66;03m# Revert\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;66;03m# model.config.pad_token_id = old_pad_token_id\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m \n\u001b[0;32m   1602\u001b[0m \u001b[38;5;66;03m# Unset a flag for generation!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1840\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2220\u001b[0m     )\n\u001b[0;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2243\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:3214\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3216\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3217\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3218\u001b[0m     outputs,\n\u001b[0;32m   3219\u001b[0m     model_kwargs,\n\u001b[0;32m   3220\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3221\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1043\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[1;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_CausalLM_fast_forward\u001b[39m(\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1026\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1040\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1051\u001b[0m         causal_mask \u001b[38;5;241m=\u001b[39m xformers\u001b[38;5;241m.\u001b[39mattn_bias\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m HAS_XFORMERS \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:978\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward_inference\u001b[1;34m(self, input_ids, past_key_values, position_ids, attention_mask)\u001b[0m\n\u001b[0;32m    970\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n\u001b[0;32m    971\u001b[0m X \u001b[38;5;241m=\u001b[39m fast_rms_layernorm_inference(\n\u001b[0;32m    972\u001b[0m     decoder_layer\u001b[38;5;241m.\u001b[39minput_layernorm,\n\u001b[0;32m    973\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     variance \u001b[38;5;241m=\u001b[39m variance,\n\u001b[0;32m    977\u001b[0m )\n\u001b[1;32m--> 978\u001b[0m X, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaAttention_fast_forward_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_prefill\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaged_attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m residual\n\u001b[0;32m    988\u001b[0m residual\u001b[38;5;241m.\u001b[39mcopy_(X) \u001b[38;5;66;03m# residual = X\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:218\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward_inference\u001b[1;34m(self, hidden_states, past_key_value, position_ids, do_prefill, attention_mask)\u001b[0m\n\u001b[0;32m    216\u001b[0m RH_Q[:,:,:,:h] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,h:]\n\u001b[0;32m    217\u001b[0m RH_Q[:,:,:,h:] \u001b[38;5;241m=\u001b[39m Qn[:,:,:,:h]\n\u001b[1;32m--> 218\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRH_Q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRH_Q\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m Qn \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cos\n\u001b[0;32m    220\u001b[0m Qn\u001b[38;5;241m.\u001b[39maddcmul_(RH_Q, sin)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    [\"Teknofest iÃ§in bilgi verebilir misin? \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate iÃ§inde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'Ä± parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ¼rkiye SÃ¼per Lig, 1972'de kurulan TÃ¼rkiye'nin en Ã¼st dÃ¼zey futbol ligi. SÃ¼per Lig, 1972-1973 sezonundan beri oynanmaktadÄ±r. 1984-85 sezonundan bu yana 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± sezonlar da vardÄ±r. 1999-2000 sezonunda 20 takÄ±mla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. SÃ¼per Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir. SÃ¼per Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan MillÃ® Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "SÃ¼per Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan MillÃ® Lig adÄ±yla oynanmÄ±ÅŸtÄ±r. MillÃ® Lig, 1972'de kurulan TÃ¼rkiye'nin en Ã¼st dÃ¼zey futbol ligi. MillÃ® Lig, 1972-1973 sezonundan beri oynanmaktadÄ±r. 1984-85 sezonundan bu yana 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± sezonlar da vardÄ±r. 1999-2000 sezonunda 20 takÄ±mla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. MillÃ® Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir. MillÃ® Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan SÃ¼per Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "SÃ¼per Lig, 1998-99 sezonu Ã¶ncesinde TÃ¼rkiye'de oynanan en Ã¼st seviyede profesyonel futbol ligi olan MillÃ® Lig'de oynayan 18 takÄ±mdan oluÅŸuyordu. 1998-99 sezonunda ise ligde 18 takÄ±m yer almÄ±ÅŸtÄ±. 2000-01 sezonundan 2011-12 sezonuna kadar SÃ¼per Lig'de oynayan 18 takÄ±mÄ±n, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. SÃ¼per Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir.\n",
      "\n",
      "Sezonlar\n",
      "1972'de kurulan TÃ¼rkiye'nin en Ã¼st dÃ¼zey futbol ligi. SÃ¼per Lig, 1972-1973 sezonundan beri oynanmaktadÄ±r. 1984-85 sezonundan bu yana 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± sezonlar da vardÄ±r. 1999-2000 sezonunda 20 takÄ±mla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. SÃ¼per Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir. SÃ¼per Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan MillÃ® Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "1998-99 sezonu Ã¶ncesinde TÃ¼rkiye'de oynanan en Ã¼st seviyede profesyonel futbol ligi olan MillÃ® Lig'de oynayan 18 takÄ±mdan oluÅŸuyordu. 1998-99 sezonunda ise ligde 18 takÄ±m yer almÄ±ÅŸtÄ±. 2000-01 sezonundan 2011-12 sezonuna kadar SÃ¼per Lig'de oynayan 18 takÄ±mÄ±n, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. SÃ¼per Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir.\n",
      "\n",
      "SÃ¼per Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan MillÃ® Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "SÃ¼per Lig, 1972'de kurulan TÃ¼rkiye'nin en Ã¼st dÃ¼zey futbol ligi. MillÃ® Lig, 1972-1973 sezonundan beri oynanmaktadÄ±r. 1984-85 sezonundan bu yana 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± sezonlar da vardÄ±r. 1999-2000 sezonunda 20 takÄ±mla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. MillÃ® Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir. MillÃ® Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan SÃ¼per Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "1998-99 sezonu Ã¶ncesinde TÃ¼rkiye'de oynanan en Ã¼st seviyede profesyonel futbol ligi olan SÃ¼per Lig'de oynayan 18 takÄ±mdan oluÅŸuyordu. 1998-99 sezonunda ise ligde 18 takÄ±m yer almÄ±ÅŸtÄ±. 2000-01 sezonundan 2011-12 sezonuna kadar MillÃ® Lig'de oynayan 18 takÄ±mÄ±n, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. MillÃ® Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir.\n",
      "\n",
      "MillÃ® Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan SÃ¼per Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "1972'de kurulan TÃ¼rkiye'nin en Ã¼st dÃ¼zey futbol ligi. MillÃ® Lig, 1972-1973 sezonundan beri oynanmaktadÄ±r. 1984-85 sezonundan bu yana 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± sezonlar da vardÄ±r. 1999-2000 sezonunda 20 takÄ±mla oynanan ligde, 2000-01 sezonundan 2011-12 sezonuna kadar 18 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. MillÃ® Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir. MillÃ® Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan SÃ¼per Lig adÄ±yla oynanmÄ±ÅŸtÄ±r.\n",
      "\n",
      "1998-99 sezonu Ã¶ncesinde TÃ¼rkiye'de oynanan en Ã¼st seviyede profesyonel futbol ligi olan MillÃ® Lig'de oynayan 18 takÄ±mdan oluÅŸuyordu. 1998-99 sezonunda ise ligde 18 takÄ±m yer almÄ±ÅŸtÄ±. 2000-01 sezonundan 2011-12 sezonuna kadar MillÃ® Lig'de oynayan 18 takÄ±mÄ±n, 2012-13 sezonundan beri 16 takÄ±mÄ±n oynadÄ±ÄŸÄ± ligde oynanmaktadÄ±r. MillÃ® Lig, TÃ¼rkiye Futbol Federasyonu (TFF) tarafÄ±ndan organize edilmektedir.\n",
      "\n",
      "MillÃ® Lig, 2001-2002 sezonuna kadar TÃ¼rkiye'nin en Ã¼st seviye ligi olan SÃ¼per Lig adÄ±yla\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer  # Yeni eklenen kÄ±sÄ±m\n",
    "import torch\n",
    "# alpaca_prompt = YukarÄ±dan kopyalandÄ±ÄŸÄ±nÄ± varsayÄ±yorum\n",
    "# Alpaca formatÄ±nda prompt ÅŸablonunun \"instruction\", \"input\", \"response\" alanlarÄ±nÄ± iÃ§erdiÄŸini kabul ediyorum\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",  # Path to the base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "# Modeli tahmin moduna geÃ§irme\n",
    "FastLanguageModel.for_inference(model)  # Modeli inference iÃ§in hazÄ±rlar, Unslothâ€™un 2 kat hÄ±zlÄ± yerel tahmin Ã¶zelliÄŸini etkinleÅŸtirir\n",
    "\n",
    "# Streamer'Ä± oluÅŸtur\n",
    "streamer = TextStreamer(\n",
    "    tokenizer=tokenizer,\n",
    "    skip_prompt=True,       # BaÅŸlangÄ±Ã§ promptunu gÃ¶sterme\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# GiriÅŸi hazÄ±rlama ve tokenize etme\n",
    "inputs = tokenizer(                     # Tokenizer ile giriÅŸ metnini tokenâ€™lara Ã§evirir\n",
    "    [                                   # Tek bir promptâ€™u liste iÃ§inde iÅŸler\n",
    "        alpaca_prompt.format(           # Alpaca ÅŸablonunu belirli deÄŸerlerle doldurur\n",
    "            \"TÃ¼rkiye SÃ¼per Lig futbol takÄ±mlarÄ± hakÄ±nda bilgi ver.\",  # Talimat: MaÃ§ sonuÃ§larÄ±nÄ± listele ve aÃ§Ä±klama ekle\n",
    "            \"\",                         # GiriÅŸ: BoÅŸ bÄ±rakÄ±ldÄ±, model kendi bilgisine dayanacak\n",
    "            \"\",                         # Ã‡Ä±kÄ±ÅŸ: BoÅŸ bÄ±rakÄ±lÄ±r, modelin Ã¼retmesi iÃ§in\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"                 # Ã‡Ä±ktÄ±yÄ± PyTorch tensÃ¶r formatÄ±nda dÃ¶ndÃ¼rÃ¼r\n",
    ").to(\"cuda\")                            # TensÃ¶rleri CUDA (GPU) belleÄŸine taÅŸÄ±r\n",
    "\n",
    "# Model ile tahmin yapma\n",
    "outputs = model.generate(               # Modeli kullanarak metin Ã¼retir\n",
    "    **inputs,                           # Tokenize edilmiÅŸ giriÅŸleri modele verir\n",
    "    max_new_tokens=1024,                 # Ãœretilecek maksimum yeni token sayÄ±sÄ±: Daha fazla ayrÄ±ntÄ± iÃ§in 512\n",
    "    use_cache=True                      # Ã–nbellek kullanÄ±mÄ±nÄ± etkinleÅŸtirir, hÄ±zÄ± artÄ±rÄ±r\n",
    ")\n",
    "\n",
    "# Generate iÃ§inde streamer kullan\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    streamer=streamer,      # Streamer'Ä± parametre olarak ekle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting Werkzeug>=3.1 (from Flask)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask) (3.1.5)\n",
      "Collecting itsdangerous>=2.2 (from Flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Flask) (8.1.8)\n",
      "Collecting blinker>=1.9 (from Flask)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click>=8.1.3->Flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mehmet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Jinja2>=3.1.2->Flask) (2.1.5)\n",
      "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: Werkzeug, itsdangerous, blinker, Flask\n",
      "Successfully installed Flask-3.1.0 Werkzeug-3.1.3 blinker-1.9.0 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:15<00:00,  1.28s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:40] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:02:52] \"GET /generate?prompt=Merhaba HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:04:34] \"GET /generate?prompt=FenerbahÃ§e%20hakkÄ±nda%20bilgi%20ver. HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:04:46] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:05:04] \"GET /generate?prompt=FenerbahÃ§e%20hakkÄ±nda%20bilgi%20ver HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:05:15] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'Ä± global olarak yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "        self.current_text = \"\"\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.current_text += text\n",
    "        # Yeni satÄ±r veya noktaya gÃ¶re bÃ¶l\n",
    "        if '\\n' in text or '. ' in text:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.current_text = \"\"\n",
    "        elif stream_end:\n",
    "            self.queue.put(self.current_text)\n",
    "            self.queue.put(None)  # BitiÅŸ iÅŸareti\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                const eventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                eventSource.onmessage = function(e) {\n",
    "                    if (e.data === 'DONE') {\n",
    "                        eventSource.close();\n",
    "                        return;\n",
    "                    }\n",
    "                    output.innerHTML += e.data;\n",
    "                    window.scrollTo(0, document.body.scrollHeight);\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "    \n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        model.generate(\n",
    "            **inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=2048,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        response_queue.put(None)  # Ä°ÅŸlem bitti sinyali\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:22<00:00,  1.84s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:10:41] \"GET /generate?prompt=FenerbahÃ§e%20nasÄ±l%20bir%20takÄ±m? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:07] \"GET /generate?prompt=SÃ¼per%20lig%20nasÄ±ldÄ±r? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:11:34] \"GET /generate?prompt=SÃ¼per%20Lig%20maÃ§larÄ±%20skorlarÄ±%20bu%20hafta HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'Ä± global olarak yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yÃ¶netimi iÃ§in global deÄŸiÅŸkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            \n",
    "            function generate() {\n",
    "                const prompt = document.getElementById('prompt').value;\n",
    "                const output = document.getElementById('output');\n",
    "                output.innerHTML = '';\n",
    "                \n",
    "                // Ã–nceki baÄŸlantÄ±yÄ± kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                currentEventSource.onmessage = function(e) {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        output.innerHTML += e.data;\n",
    "                        window.scrollTo(0, document.body.scrollHeight);\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "        </script>\n",
    "        <textarea id=\"prompt\" rows=\"4\" cols=\"50\"></textarea><br>\n",
    "        <button onclick=\"generate()\">Generate</button>\n",
    "        <div id=\"output\" style=\"white-space: pre-wrap; margin-top: 20px;\"></div>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Ã–nceki iÅŸlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:14<00:00,  1.24s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://10.106.250.143:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Mar/2025 01:15:05] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Mar/2025 01:15:05] \"GET /generate?prompt=SÃ¼per%20Lig%20maÃ§larÄ±%20skorlarÄ±%20bu%20hafta HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'Ä± global olarak yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yÃ¶netimi iÃ§in global deÄŸiÅŸkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>AI Chat</title>\n",
    "        <style>\n",
    "            :root {\n",
    "                --primary-color: #2563eb;\n",
    "                --user-bg: #3b82f6;\n",
    "                --bot-bg: #f1f5f9;\n",
    "                --text-color: #1e293b;\n",
    "            }\n",
    "\n",
    "            * {\n",
    "                box-sizing: border-box;\n",
    "                font-family: 'Segoe UI', sans-serif;\n",
    "            }\n",
    "\n",
    "            body {\n",
    "                margin: 0;\n",
    "                background: #f8fafc;\n",
    "                height: 100vh;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "            }\n",
    "\n",
    "            .chat-container {\n",
    "                flex: 1;\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                width: 100%;\n",
    "                padding: 20px;\n",
    "                overflow-y: auto;\n",
    "            }\n",
    "\n",
    "            .message {\n",
    "                max-width: 70%;\n",
    "                margin: 10px 0;\n",
    "                padding: 15px 20px;\n",
    "                border-radius: 20px;\n",
    "                animation: fadeIn 0.3s ease-out;\n",
    "                transform-origin: bottom;\n",
    "                position: relative;\n",
    "            }\n",
    "\n",
    "            .user-message {\n",
    "                background: var(--user-bg);\n",
    "                color: white;\n",
    "                margin-left: auto;\n",
    "                border-bottom-right-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .bot-message {\n",
    "                background: var(--bot-bg);\n",
    "                color: var(--text-color);\n",
    "                margin-right: auto;\n",
    "                border-bottom-left-radius: 5px;\n",
    "            }\n",
    "\n",
    "            .typing-indicator {\n",
    "                display: inline-flex;\n",
    "                padding: 10px 15px;\n",
    "                background: var(--bot-bg);\n",
    "                border-radius: 20px;\n",
    "                margin: 5px 0;\n",
    "            }\n",
    "\n",
    "            .dot {\n",
    "                width: 8px;\n",
    "                height: 8px;\n",
    "                margin: 0 3px;\n",
    "                background: #64748b;\n",
    "                border-radius: 50%;\n",
    "                animation: bounce 1.4s infinite;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(2) {\n",
    "                animation-delay: 0.2s;\n",
    "            }\n",
    "\n",
    "            .dot:nth-child(3) {\n",
    "                animation-delay: 0.4s;\n",
    "            }\n",
    "\n",
    "            @keyframes fadeIn {\n",
    "                from {\n",
    "                    opacity: 0;\n",
    "                    transform: translateY(10px);\n",
    "                }\n",
    "                to {\n",
    "                    opacity: 1;\n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            @keyframes bounce {\n",
    "                0%, 80%, 100% { \n",
    "                    transform: translateY(0);\n",
    "                }\n",
    "                40% {\n",
    "                    transform: translateY(-8px);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            .input-container {\n",
    "                background: white;\n",
    "                padding: 20px;\n",
    "                box-shadow: 0 -2px 10px rgba(0,0,0,0.05);\n",
    "            }\n",
    "\n",
    "            .input-wrapper {\n",
    "                max-width: 800px;\n",
    "                margin: 0 auto;\n",
    "                display: flex;\n",
    "                gap: 10px;\n",
    "            }\n",
    "\n",
    "            textarea {\n",
    "                flex: 1;\n",
    "                padding: 12px 16px;\n",
    "                border: 2px solid #e2e8f0;\n",
    "                border-radius: 12px;\n",
    "                resize: none;\n",
    "                font-size: 16px;\n",
    "                transition: border-color 0.2s;\n",
    "            }\n",
    "\n",
    "            textarea:focus {\n",
    "                outline: none;\n",
    "                border-color: var(--primary-color);\n",
    "            }\n",
    "\n",
    "            button {\n",
    "                background: var(--primary-color);\n",
    "                color: white;\n",
    "                border: none;\n",
    "                padding: 12px 24px;\n",
    "                border-radius: 12px;\n",
    "                cursor: pointer;\n",
    "                font-weight: 600;\n",
    "                transition: transform 0.2s, background 0.2s;\n",
    "            }\n",
    "\n",
    "            button:hover {\n",
    "                background: #1d4ed8;\n",
    "                transform: translateY(-1px);\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"chat-container\" id=\"chatContainer\">\n",
    "            <div class=\"message bot-message\">\n",
    "                <div class=\"typing-indicator\">\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                    <div class=\"dot\"></div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"input-container\">\n",
    "            <div class=\"input-wrapper\">\n",
    "                <textarea id=\"prompt\" placeholder=\"MesajÄ±nÄ±zÄ± yazÄ±n...\" rows=\"1\"></textarea>\n",
    "                <button onclick=\"generate()\">GÃ¶nder</button>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <script>\n",
    "            let currentEventSource = null;\n",
    "            const chatContainer = document.getElementById('chatContainer');\n",
    "            const textarea = document.getElementById('prompt');\n",
    "\n",
    "            // Otomatik textarea yÃ¼ksekliÄŸi ayarÄ±\n",
    "            textarea.addEventListener('input', () => {\n",
    "                textarea.style.height = 'auto';\n",
    "                textarea.style.height = textarea.scrollHeight + 'px';\n",
    "            });\n",
    "\n",
    "            function addMessage(text, isUser) {\n",
    "                const messageDiv = document.createElement('div');\n",
    "                messageDiv.className = `message ${isUser ? 'user-message' : 'bot-message'}`;\n",
    "                \n",
    "                if(!isUser) {\n",
    "                    const typingIndicator = document.querySelector('.typing-indicator');\n",
    "                    if(typingIndicator) typingIndicator.remove();\n",
    "                }\n",
    "                \n",
    "                messageDiv.innerHTML = text.replace(/\\n/g, '<br>');\n",
    "                chatContainer.appendChild(messageDiv);\n",
    "                messageDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "            }\n",
    "\n",
    "            function generate() {\n",
    "                const prompt = textarea.value.trim();\n",
    "                if(!prompt) return;\n",
    "\n",
    "                // Ã–nceki baÄŸlantÄ±yÄ± kapat\n",
    "                if(currentEventSource) currentEventSource.close();\n",
    "                \n",
    "                // KullanÄ±cÄ± mesajÄ±nÄ± ekle\n",
    "                addMessage(prompt, true);\n",
    "                textarea.value = '';\n",
    "                \n",
    "                // Typing indicator ekle\n",
    "                const typingDiv = document.createElement('div');\n",
    "                typingDiv.className = 'message bot-message';\n",
    "                typingDiv.innerHTML = `\n",
    "                    <div class=\"typing-indicator\">\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                        <div class=\"dot\"></div>\n",
    "                    </div>\n",
    "                `;\n",
    "                chatContainer.appendChild(typingDiv);\n",
    "                typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "\n",
    "                // Yeni baÄŸlantÄ± oluÅŸtur\n",
    "                currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "                \n",
    "                let botResponse = '';\n",
    "                currentEventSource.onmessage = (e) => {\n",
    "                    if(e.data === 'DONE') {\n",
    "                        currentEventSource.close();\n",
    "                    } else {\n",
    "                        botResponse += e.data;\n",
    "                        typingDiv.innerHTML = botResponse.replace(/\\n/g, '<br>');\n",
    "                        typingDiv.scrollIntoView({ behavior: 'smooth' });\n",
    "                    }\n",
    "                };\n",
    "            }\n",
    "\n",
    "            // Enter tuÅŸu desteÄŸi\n",
    "            textarea.addEventListener('keydown', (e) => {\n",
    "                if(e.key === 'Enter' && !e.shiftKey) {\n",
    "                    e.preventDefault();\n",
    "                    generate();\n",
    "                }\n",
    "            });\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    ''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    # Ã–nceki iÅŸlemi durdur\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "    \n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)]\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
