{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:30<00:00,  2.54s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.33.183:5000\n",
      "Press CTRL+C to quit\n",
      "192.168.33.183 - - [10/Mar/2025 10:07:23] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.33.183 - - [10/Mar/2025 10:07:23] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.33.183 - - [10/Mar/2025 10:07:34] \"GET /generate?prompt=Mehmet%20AKINOL%20Kimdir%20?%20hakkÄ±nda%20bilgi%20ver. HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Model ve tokenizer'Ä± yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Thread yÃ¶netimi iÃ§in global deÄŸiÅŸkenler\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"tr\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\" />\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "  <title>ChatGPT - Chatbot</title>\n",
    "  <style>\n",
    "    body {\n",
    "      margin: 0;\n",
    "      font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif;\n",
    "      background-color: #f7f7f8;\n",
    "      display: flex;\n",
    "      flex-direction: column;\n",
    "      height: 100vh;\n",
    "      color: #333;\n",
    "    }\n",
    "    .container {\n",
    "      display: flex;\n",
    "      flex-direction: column;\n",
    "      flex-grow: 1;\n",
    "      max-width: 800px;\n",
    "      margin: 0 auto;\n",
    "      padding: 20px;\n",
    "    }\n",
    "    .header {\n",
    "      text-align: center;\n",
    "      margin-bottom: 10px;\n",
    "      font-size: 24px;\n",
    "      font-weight: bold;\n",
    "    }\n",
    "    .chat-area {\n",
    "      flex-grow: 1;\n",
    "      overflow-y: auto;\n",
    "      padding: 20px;\n",
    "      background: #fff;\n",
    "      border-radius: 8px;\n",
    "      box-shadow: 0 0 10px rgba(0,0,0,0.05);\n",
    "    }\n",
    "    .message {\n",
    "      display: block;\n",
    "      padding: 10px 15px;\n",
    "      margin: 10px 0;\n",
    "      border-radius: 8px;\n",
    "      max-width: 75%;\n",
    "      line-height: 1.5;\n",
    "      white-space: pre-wrap;\n",
    "      word-wrap: break-word;\n",
    "    }\n",
    "    .message.user {\n",
    "      background: #dcf8c6;\n",
    "      align-self: flex-end;\n",
    "    }\n",
    "    .message.ai {\n",
    "      background: #e1e1e1;\n",
    "      align-self: flex-start;\n",
    "    }\n",
    "    .input-container {\n",
    "      display: flex;\n",
    "      margin-top: 10px;\n",
    "      background: #fff;\n",
    "      border-radius: 8px;\n",
    "      box-shadow: 0 0 10px rgba(0,0,0,0.05);\n",
    "      padding: 10px;\n",
    "    }\n",
    "    .input-container textarea {\n",
    "      flex-grow: 1;\n",
    "      border: none;\n",
    "      resize: none;\n",
    "      font-size: 16px;\n",
    "      padding: 10px;\n",
    "      outline: none;\n",
    "    }\n",
    "    .input-container button {\n",
    "      background: #007bff;\n",
    "      border: none;\n",
    "      color: white;\n",
    "      padding: 10px 20px;\n",
    "      border-radius: 8px;\n",
    "      margin-left: 10px;\n",
    "      cursor: pointer;\n",
    "      font-size: 16px;\n",
    "    }\n",
    "    .input-container button:hover {\n",
    "      background: #0056b3;\n",
    "    }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <div class=\"header\">ChatGPT</div>\n",
    "    <div id=\"chatArea\" class=\"chat-area\"></div>\n",
    "    <div class=\"input-container\">\n",
    "      <textarea id=\"prompt\" rows=\"2\" placeholder=\"Sorunuzu buraya yazÄ±n...\"></textarea>\n",
    "      <button onclick=\"sendPrompt()\">GÃ¶nder</button>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let currentEventSource = null;\n",
    "\n",
    "    function sendPrompt() {\n",
    "      const prompt = document.getElementById('prompt').value.trim();\n",
    "      if (!prompt) return;\n",
    "\n",
    "      const chatArea = document.getElementById('chatArea');\n",
    "\n",
    "      // KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± ekle\n",
    "      const userMessage = document.createElement('div');\n",
    "      userMessage.classList.add('message', 'user');\n",
    "      userMessage.textContent = prompt;\n",
    "      chatArea.appendChild(userMessage);\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "\n",
    "      // GiriÅŸ alanÄ±nÄ± temizle\n",
    "      document.getElementById('prompt').value = '';\n",
    "\n",
    "      // Ã–nceki EventSource varsa kapat\n",
    "      if (currentEventSource) {\n",
    "        currentEventSource.close();\n",
    "      }\n",
    "\n",
    "      // Yapay zeka mesajÄ± iÃ§in bir konteyner oluÅŸtur\n",
    "      const aiMessage = document.createElement('div');\n",
    "      aiMessage.classList.add('message', 'ai');\n",
    "      chatArea.appendChild(aiMessage);\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "\n",
    "      // Yeni EventSource oluÅŸtur\n",
    "      currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "      currentEventSource.onmessage = function(e) {\n",
    "        if (e.data === 'DONE') {\n",
    "          currentEventSource.close();\n",
    "          return;\n",
    "        }\n",
    "        aiMessage.textContent += e.data;\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "      };\n",
    "    }\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "        stop_event = current_stop_event\n",
    "\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer=tokenizer,\n",
    "        queue=response_queue,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=1024,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                stopping_criteria=[StopGenerationCriteria(stop_event)],\n",
    "                repetition_penalty=1.2,  # Tekrar cezasÄ± eklendi\n",
    "                temperature=0.7,         # Rastgelelik seviyesi\n",
    "                top_p=0.9,               # NÃ¼kleus Ã¶rnekleme\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def event_stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None:\n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "\n",
    "    return Response(event_stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:25<00:00,  2.13s/it]\n",
      "Unsloth 2025.2.15 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\peft\\peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.42.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.43.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.44.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.45.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.46.mlp.down_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.o_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.self_attn.o_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.gate_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.gate_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.up_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.up_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.down_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.47.mlp.down_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.33.183:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Mar/2025 13:12:39] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:12:39] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:12:47] \"GET /generate?prompt=Merhaba HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:16:07] \"GET /generate?prompt=NasÄ±lsÄ±n? HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:16:18] \"GET /generate?prompt=Merhaba HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:17:17] \"GET /generate?prompt=Bu%20hafta%20FenerbahÃ§e%20maÃ§larÄ± HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Mar/2025 13:18:05] \"GET /generate?prompt=SaÄŸlÄ±k%20Ã¼zerine%20haber%20yaz HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#gerÃ§ek Ã§alÄ±ÅŸan\n",
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Quantization ayarlarÄ±\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "# Model ve tokenizer'Ä± yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼kle\n",
    "model = PeftModel.from_pretrained(model, \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Thread yÃ¶netimi\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "def format_alpaca_prompt(instruction):\n",
    "    return f\"\"\"AÅŸaÄŸÄ±da bir gÃ¶revi aÃ§Ä±klayan bir talimat bulunmaktadÄ±r. Ä°steÄŸi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.\n",
    "\n",
    "### Talimat:\n",
    "{instruction}\n",
    "\n",
    "### YanÄ±t:\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"tr\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Alpaca Chat</title>\n",
    "  <style>\n",
    "    /* Stil tanÄ±mlarÄ± aynÄ± kalÄ±yor */\n",
    "    .container { max-width: 800px; margin: 0 auto; padding: 20px; height: 100vh; display: flex; flex-direction: column; }\n",
    "    .chat-area { flex-grow: 1; overflow-y: auto; background: #fff; border-radius: 8px; padding: 20px; margin: 10px 0; }\n",
    "    .message { max-width: 75%; padding: 10px; margin: 5px 0; border-radius: 8px; }\n",
    "    .user { background: #dcf8c6; margin-left: auto; }\n",
    "    .ai { background: #f0f0f0; }\n",
    "    .input-container { display: flex; gap: 10px; }\n",
    "    textarea { flex-grow: 1; padding: 10px; border-radius: 8px; border: 1px solid #ddd; }\n",
    "    button { background: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 8px; cursor: pointer; }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <h1>Alpaca Chat</h1>\n",
    "    <div id=\"chatArea\" class=\"chat-area\"></div>\n",
    "    <div class=\"input-container\">\n",
    "      <textarea id=\"prompt\" rows=\"3\" placeholder=\"Alpaca modeli iÃ§in sorunuzu girin...\"></textarea>\n",
    "      <button onclick=\"sendPrompt()\">GÃ¶nder</button>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let currentEventSource = null;\n",
    "\n",
    "    function sendPrompt() {\n",
    "      const prompt = document.getElementById('prompt').value.trim();\n",
    "      if (!prompt) return;\n",
    "\n",
    "      const chatArea = document.getElementById('chatArea');\n",
    "      \n",
    "      // KullanÄ±cÄ± mesajÄ±nÄ± ekle\n",
    "      const userDiv = document.createElement('div');\n",
    "      userDiv.className = 'message user';\n",
    "      userDiv.textContent = prompt;\n",
    "      chatArea.appendChild(userDiv);\n",
    "\n",
    "      // AI mesaj konteyneri\n",
    "      const aiDiv = document.createElement('div');\n",
    "      aiDiv.className = 'message ai';\n",
    "      chatArea.appendChild(aiDiv);\n",
    "      \n",
    "      // Ã–nceki baÄŸlantÄ±yÄ± kapat\n",
    "      if(currentEventSource) currentEventSource.close();\n",
    "\n",
    "      // Yeni istek baÅŸlat\n",
    "      currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "      \n",
    "      currentEventSource.onmessage = (e) => {\n",
    "        if(e.data === 'DONE') {\n",
    "          currentEventSource.close();\n",
    "          aiDiv.innerHTML += '<div style=\"color: #666; font-size: 0.8em\">â–¼ Cevap TamamlandÄ±</div>';\n",
    "          return;\n",
    "        }\n",
    "        aiDiv.textContent += e.data;\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "      };\n",
    "      \n",
    "      document.getElementById('prompt').value = '';\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "    }\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "    \n",
    "    stop_event = current_stop_event\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    # Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    formatted_prompt = format_alpaca_prompt(prompt)\n",
    "    \n",
    "    # Tokenizer ayarlarÄ±\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 2048,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Streamer ayarlarÄ±\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer = tokenizer,\n",
    "        queue = response_queue,\n",
    "        skip_prompt = True,\n",
    "        skip_special_tokens = True,\n",
    "        clean_up_tokenization_spaces = True,\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer = streamer,\n",
    "                max_new_tokens = 2048,\n",
    "                temperature = 0.6,\n",
    "                top_p = 0.9,\n",
    "                top_k = 40,\n",
    "                do_sample = True,\n",
    "                repetition_penalty = 1.15,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                pad_token_id = tokenizer.pad_token_id,\n",
    "                stopping_criteria = [StopGenerationCriteria(stop_event)],\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None: \n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "    \n",
    "    return Response(stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     13\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     bnb_4bit_quant_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     bnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[0;32m     16\u001b[0m     bnb_4bit_use_double_quant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Model ve tokenizer'Ä± yÃ¼kle\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMehmet\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDenizhan2\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmodel_egitim\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-1500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼kle\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMehmet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDenizhan2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel_egitim\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcheckpoint-1500\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\loader.py:296\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\qwen2.py:87\u001b[0m, in \u001b[0;36mFastQwen2Model.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     74\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     86\u001b[0m ):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFastQwen2Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1796\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[1;32m-> 1796\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[0;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1809\u001b[0m         load_vllm,\n\u001b[0;32m   1810\u001b[0m         get_vllm_state_dict,\n\u001b[0;32m   1811\u001b[0m         convert_vllm_to_huggingface,\n\u001b[0;32m   1812\u001b[0m         generate_batches,\n\u001b[0;32m   1813\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4262\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Quantization ayarlarÄ±\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "# Model ve tokenizer'Ä± yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼kle\n",
    "model = PeftModel.from_pretrained(model, \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Thread yÃ¶netimi\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "def format_alpaca_prompt(instruction):\n",
    "    return f\"\"\"AÅŸaÄŸÄ±da bir gÃ¶revi aÃ§Ä±klayan bir talimat bulunmaktadÄ±r. Ä°steÄŸi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.\n",
    "\n",
    "### Talimat:\n",
    "{instruction}\n",
    "\n",
    "### YanÄ±t:\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"tr\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Alpaca Chat</title>\n",
    "  <style>\n",
    "    /* Stil tanÄ±mlarÄ± aynÄ± kalÄ±yor */\n",
    "    .container { max-width: 800px; margin: 0 auto; padding: 20px; height: 100vh; display: flex; flex-direction: column; }\n",
    "    .chat-area { flex-grow: 1; overflow-y: auto; background: #fff; border-radius: 8px; padding: 20px; margin: 10px 0; }\n",
    "    .message { max-width: 75%; padding: 10px; margin: 5px 0; border-radius: 8px; }\n",
    "    .user { background: #dcf8c6; margin-left: auto; }\n",
    "    .ai { background: #f0f0f0; }\n",
    "    .input-container { display: flex; gap: 10px; }\n",
    "    textarea { flex-grow: 1; padding: 10px; border-radius: 8px; border: 1px solid #ddd; }\n",
    "    button { background: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 8px; cursor: pointer; }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <h1>Alpaca Chat</h1>\n",
    "    <div id=\"chatArea\" class=\"chat-area\"></div>\n",
    "    <div class=\"input-container\">\n",
    "      <textarea id=\"prompt\" rows=\"3\" placeholder=\"Alpaca modeli iÃ§in sorunuzu girin...\"></textarea>\n",
    "      <button onclick=\"sendPrompt()\">GÃ¶nder</button>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let currentEventSource = null;\n",
    "\n",
    "    function sendPrompt() {\n",
    "      const prompt = document.getElementById('prompt').value.trim();\n",
    "      if (!prompt) return;\n",
    "\n",
    "      const chatArea = document.getElementById('chatArea');\n",
    "      \n",
    "      // KullanÄ±cÄ± mesajÄ±nÄ± ekle\n",
    "      const userDiv = document.createElement('div');\n",
    "      userDiv.className = 'message user';\n",
    "      userDiv.textContent = prompt;\n",
    "      chatArea.appendChild(userDiv);\n",
    "\n",
    "      // AI mesaj konteyneri\n",
    "      const aiDiv = document.createElement('div');\n",
    "      aiDiv.className = 'message ai';\n",
    "      chatArea.appendChild(aiDiv);\n",
    "      \n",
    "      // Ã–nceki baÄŸlantÄ±yÄ± kapat\n",
    "      if(currentEventSource) currentEventSource.close();\n",
    "\n",
    "      // Yeni istek baÅŸlat\n",
    "      currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "      \n",
    "      currentEventSource.onmessage = (e) => {\n",
    "        if(e.data === 'DONE') {\n",
    "          currentEventSource.close();\n",
    "          aiDiv.innerHTML += '<div style=\"color: #666; font-size: 0.8em\">â–¼ Cevap TamamlandÄ±</div>';\n",
    "          return;\n",
    "        }\n",
    "        aiDiv.textContent += e.data;\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "      };\n",
    "      \n",
    "      document.getElementById('prompt').value = '';\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "    }\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "    \n",
    "    stop_event = current_stop_event\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    # Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    formatted_prompt = format_alpaca_prompt(prompt)\n",
    "    \n",
    "    # Tokenizer ayarlarÄ±\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 2048,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Streamer ayarlarÄ±\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer = tokenizer,\n",
    "        queue = response_queue,\n",
    "        skip_prompt = True,\n",
    "        skip_special_tokens = True,\n",
    "        clean_up_tokenization_spaces = True,\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer = streamer,\n",
    "                max_new_tokens = 2048,\n",
    "                temperature = 0.6,\n",
    "                top_p = 0.9,\n",
    "                top_k = 40,\n",
    "                do_sample = True,\n",
    "                repetition_penalty = 1.15,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                pad_token_id = tokenizer.pad_token_id,\n",
    "                stopping_criteria = [StopGenerationCriteria(stop_event)],\n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None: \n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "    \n",
    "    return Response(stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A5000. Max memory: 23.988 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "C:\\Users\\Mehmet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1277: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m quantization_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     13\u001b[0m     load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     bnb_4bit_quant_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     bnb_4bit_compute_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[0;32m     16\u001b[0m     bnb_4bit_use_double_quant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Model ve tokenizer'Ä± yÃ¼kle\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMehmet\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDenizhan2\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmodel_egitim\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mcheckpoint-1500\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼kle\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mMehmet\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDenizhan2\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmodel_egitim\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcheckpoint-1500\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\loader.py:296\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\qwen2.py:87\u001b[0m, in \u001b[0;36mFastQwen2Model.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     74\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     86\u001b[0m ):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFastLlamaModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFastQwen2Model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\unsloth\\models\\llama.py:1796\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[1;32m-> 1796\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# quantization_config     = bnb_config,\u001b[39;49;00m\n\u001b[0;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1809\u001b[0m         load_vllm,\n\u001b[0;32m   1810\u001b[0m         get_vllm_state_dict,\n\u001b[0;32m   1811\u001b[0m         convert_vllm_to_huggingface,\n\u001b[0;32m   1812\u001b[0m         generate_batches,\n\u001b[0;32m   1813\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4262\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "\n",
    "from flask import Flask, Response, request, render_template_string\n",
    "from transformers import TextStreamer, StoppingCriteria, BitsAndBytesConfig\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Quantization ayarlarÄ±\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "# Model ve tokenizer'Ä± yÃ¼kle\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16,\n",
    "    load_in_4bit = True,\n",
    "    quantization_config = quantization_config,\n",
    "    device_map = \"auto\",\n",
    ")\n",
    "\n",
    "# PEFT adaptÃ¶rÃ¼nÃ¼ yÃ¼kle\n",
    "model = PeftModel.from_pretrained(model, \"C:\\\\Users\\\\Mehmet\\\\Desktop\\\\Denizhan2\\\\model_egitim\\\\checkpoint-1500\")\n",
    "FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Thread yÃ¶netimi\n",
    "current_stop_event = None\n",
    "stop_lock = threading.Lock()\n",
    "\n",
    "class StopGenerationCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_event):\n",
    "        super().__init__()\n",
    "        self.stop_event = stop_event\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_event.is_set()\n",
    "\n",
    "class WebStreamer(TextStreamer):\n",
    "    def __init__(self, tokenizer, queue, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.queue = queue\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        self.queue.put(text)\n",
    "        if stream_end:\n",
    "            self.queue.put(None)\n",
    "\n",
    "\n",
    "\n",
    "def format_alpaca_prompt(instruction):\n",
    "    return f\"\"\"AÅŸaÄŸÄ±da bir gÃ¶revi aÃ§Ä±klayan bir talimat bulunmaktadÄ±r. Ä°steÄŸi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.\n",
    "\n",
    "### Talimat:\n",
    "{instruction}\n",
    "\n",
    "### YanÄ±t:\n",
    "\"\"\"\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string('''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"tr\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <title>Alpaca Chat</title>\n",
    "  <style>\n",
    "    /* Stil tanÄ±mlarÄ± aynÄ± kalÄ±yor */\n",
    "    .container { max-width: 800px; margin: 0 auto; padding: 20px; height: 100vh; display: flex; flex-direction: column; }\n",
    "    .chat-area { flex-grow: 1; overflow-y: auto; background: #fff; border-radius: 8px; padding: 20px; margin: 10px 0; }\n",
    "    .message { max-width: 75%; padding: 10px; margin: 5px 0; border-radius: 8px; }\n",
    "    .user { background: #dcf8c6; margin-left: auto; }\n",
    "    .ai { background: #f0f0f0; }\n",
    "    .input-container { display: flex; gap: 10px; }\n",
    "    textarea { flex-grow: 1; padding: 10px; border-radius: 8px; border: 1px solid #ddd; }\n",
    "    button { background: #007bff; color: white; border: none; padding: 10px 20px; border-radius: 8px; cursor: pointer; }\n",
    "  </style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"container\">\n",
    "    <h1>Alpaca Chat</h1>\n",
    "    <div id=\"chatArea\" class=\"chat-area\"></div>\n",
    "    <div class=\"input-container\">\n",
    "      <textarea id=\"prompt\" rows=\"3\" placeholder=\"Alpaca modeli iÃ§in sorunuzu girin...\"></textarea>\n",
    "      <button onclick=\"sendPrompt()\">GÃ¶nder</button>\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <script>\n",
    "    let currentEventSource = null;\n",
    "\n",
    "    function sendPrompt() {\n",
    "      const prompt = document.getElementById('prompt').value.trim();\n",
    "      if (!prompt) return;\n",
    "\n",
    "      const chatArea = document.getElementById('chatArea');\n",
    "      \n",
    "      // KullanÄ±cÄ± mesajÄ±nÄ± ekle\n",
    "      const userDiv = document.createElement('div');\n",
    "      userDiv.className = 'message user';\n",
    "      userDiv.textContent = prompt;\n",
    "      chatArea.appendChild(userDiv);\n",
    "\n",
    "      // AI mesaj konteyneri\n",
    "      const aiDiv = document.createElement('div');\n",
    "      aiDiv.className = 'message ai';\n",
    "      chatArea.appendChild(aiDiv);\n",
    "      \n",
    "      // Ã–nceki baÄŸlantÄ±yÄ± kapat\n",
    "      if(currentEventSource) currentEventSource.close();\n",
    "\n",
    "      // Yeni istek baÅŸlat\n",
    "      currentEventSource = new EventSource(`/generate?prompt=${encodeURIComponent(prompt)}`);\n",
    "      \n",
    "      currentEventSource.onmessage = (e) => {\n",
    "        if(e.data === 'DONE') {\n",
    "          currentEventSource.close();\n",
    "          aiDiv.innerHTML += '<div style=\"color: #666; font-size: 0.8em\">â–¼ Cevap TamamlandÄ±</div>';\n",
    "          return;\n",
    "        }\n",
    "        aiDiv.textContent += e.data;\n",
    "        chatArea.scrollTop = chatArea.scrollHeight;\n",
    "      };\n",
    "      \n",
    "      document.getElementById('prompt').value = '';\n",
    "      chatArea.scrollTop = chatArea.scrollHeight;\n",
    "    }\n",
    "  </script>\n",
    "</body>\n",
    "</html>\n",
    "''')\n",
    "\n",
    "@app.route('/generate')\n",
    "def generate():\n",
    "    global current_stop_event\n",
    "    \n",
    "    with stop_lock:\n",
    "        if current_stop_event:\n",
    "            current_stop_event.set()\n",
    "        current_stop_event = threading.Event()\n",
    "    \n",
    "    stop_event = current_stop_event\n",
    "    prompt = request.args.get('prompt', '')\n",
    "    response_queue = queue.Queue()\n",
    "\n",
    "    # Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    formatted_prompt = format_alpaca_prompt(prompt)\n",
    "    \n",
    "    # Tokenizer ayarlarÄ±\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 2048,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Streamer ayarlarÄ±\n",
    "    streamer = WebStreamer(\n",
    "        tokenizer = tokenizer,\n",
    "        queue = response_queue,\n",
    "        skip_prompt = True,\n",
    "        skip_special_tokens = True,\n",
    "        clean_up_tokenization_spaces = True,\n",
    "    )\n",
    "\n",
    "    def generation_task():\n",
    "        try:\n",
    "            model.generate(\n",
    "                **inputs,\n",
    "                streamer = streamer,\n",
    "                max_new_tokens = 2048,\n",
    "                temperature = 0.6,\n",
    "                top_p = 0.9,\n",
    "                top_k = 40,\n",
    "                do_sample = True,\n",
    "                repetition_penalty = 1.15,\n",
    "                eos_token_id = tokenizer.eos_token_id,\n",
    "                pad_token_id = tokenizer.pad_token_id,\n",
    "                stopping_criteria = [StopGenerationCriteria(stop_event)],\n",
    "               \n",
    "                 \n",
    "            )\n",
    "        finally:\n",
    "            response_queue.put(None)\n",
    "\n",
    "    threading.Thread(target=generation_task).start()\n",
    "\n",
    "    def stream():\n",
    "        while True:\n",
    "            chunk = response_queue.get()\n",
    "            if chunk is None: \n",
    "                yield \"data: DONE\\n\\n\"\n",
    "                break\n",
    "            yield f\"data: {chunk}\\n\\n\"\n",
    "    \n",
    "    return Response(stream(), mimetype=\"text/event-stream\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
