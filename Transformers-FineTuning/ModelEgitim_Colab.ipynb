{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BzK5MqC3NLZ"
      },
      "source": [
        "## Yol Belirleme\n",
        "Bu arada Google Drive içinde çalışacağınız klasörü belirlemeniz gerekmektedir. Veri seti düzenleme veya model eğitimi için burayı yeniden çalıştırın."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8V0KIx-29cO"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l53mto03Loj"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Veri-Kazima/colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiFoeWVP3Mq0"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiR4P1q088rC"
      },
      "source": [
        "## Gerekli Kütüphaneleri Yükleyin\n",
        "Burada model eğitimi, model çalıştırma için buraya yeniden gelebilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5v31Cnq4IFI"
      },
      "outputs": [],
      "source": [
        "#İlgili kütüphanelerin Kullanımı\n",
        "!pip install transformers torch peft datasets accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y30QewwhGCiB"
      },
      "outputs": [],
      "source": [
        "#Bazı durumlarda PyTorch için uygun olan CUDA sürümüne göre kurulum yapmanız gerekebilir.\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir0jPeqz4JTs"
      },
      "outputs": [],
      "source": [
        "#Bazen Bitsanbtyes kütüphanesinde sorun olabiliyor, kaldırmayı deneyin\n",
        "!pip uninstall bitsandbytes -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrpSy9af4b5E"
      },
      "outputs": [],
      "source": [
        "#Bitsanbytes kurululumunu yeniden yapın. Daha sonra Google Colab'ı yeniden başlatın.\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vKBRUNX3Zi-"
      },
      "source": [
        "## Veri Seti Kullanma\n",
        "Burada daha önce Selenium kullanılarak çeşitli internet sitelerinden kazınan CSV formatında veriler yer almaktadır. Bu veriler ALPACA formatında JSON olarak yeniden kaydedeceğiz. Kaggle ile indirdiğimiz veri setinin ismini ise değiştirin ve bunu yapın : Tum-Veri-Seti-Nisan2024-10.csv\n",
        "\n",
        "Eğer veri setinin ismini kendiniz belirlemek isterseniz koddaki ilgili alana dosya yolunu tam olarak yazınız."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiTlIrpSImzb"
      },
      "outputs": [],
      "source": [
        "#Veriyi indirin, burada sizin için daha öncesinde topladığım 114.000 satır veri vardır. Bu veriyi Google Colab içine ekleyin.\n",
        "#https://www.kaggle.com/datasets/denizhanahin/tr-dataset114k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfiI_vYiI7_U"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"denizhanahin/tr-dataset114k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go-IqmD23xFZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def csv_to_alpaca(csv_file_path, output_json_path):\n",
        "    \"\"\"\n",
        "    CSV dosyasını Alpaca formatına dönüştürür ve JSON dosyasına kaydeder.\n",
        "\n",
        "    Args:\n",
        "        csv_file_path (str): CSV dosyasının yolu.\n",
        "        output_json_path (str): Oluşturulacak JSON dosyasının yolu.\n",
        "    \"\"\"\n",
        "\n",
        "    # CSV dosyasını pandas DataFrame'ine oku\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "\n",
        "    # Alpaca formatına dönüştür\n",
        "    alpaca_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        alpaca_entry = {\n",
        "            \"instruction\": row[\"Baslik\"],  # Başlık bilgisini \"instruction\" alanına ekle\n",
        "            \"input\": row[\"Ozet\"],  # Özet bilgisini \"input\" alanına ekle\n",
        "            \"output\": row[\"Icerik\"]  # İçerik bilgisini \"output\" alanına ekle\n",
        "        }\n",
        "        alpaca_data.append(alpaca_entry)\n",
        "\n",
        "    # JSON dosyasına kaydet\n",
        "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(alpaca_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Örnek kullanım\n",
        "csv_file_path = \"/content/drive/MyDrive/Veri-Kazima/Model_Eğitim/Tum-Veri-Seti-Nisan2024-10.csv\"  # CSV dosyanızın adı\n",
        "output_json_path = \"1alpaca_format.json\"  # Oluşturulacak JSON dosyasının adı\n",
        "csv_to_alpaca(csv_file_path, output_json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmXqzOGv33VL"
      },
      "source": [
        "## Model Eğitimi - GPU\n",
        "Google Colab üzerinde kullandığınız GPU birimini kullanarak model eğitimi yapılacaktır."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4httb6G4xvt"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 0. Gerekli Kütüphaneler\n",
        "# ----------------------------------------------\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 1. Google Drive Bağlantısı\n",
        "# ----------------------------------------------\n",
        "drive.mount('/content/drive')\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/Veri-Kazima/colab/gemma2_model_yeni\" #Model verilerini kayıt edeceğiniz klasörü seçin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejuudtwT5Jkn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"senin token\"  # Hugging Face üzerinden ince ayar için kullanacağımız modeli indirmek için Hugging Face tokenı gerekmektedir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7vG6eBO5V-m"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 2. Tokenizer ve Padding Token Ayarları\n",
        "# ----------------------------------------------\n",
        "model_name = \"google/gemma-2b\" #\"meta-llama/Meta-Llama-3-8B\" #burada ise sahip olduğunuz RAM ve GPU RAM miktarına göre seçin. Ben kısa süreli eğitim için düşük parametreli model seçtim.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"senin token\") # Hugging Face üzerinden ince ayar için kullanacağımız modeli indirmek için Hugging Face tokenı gerekmektedir\n",
        "\n",
        "# Padding token ekleme (Mistral için gerekli)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMIk66vmcDHo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Model ve tokenizer'ı yükle\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Yerel dizine kaydet\n",
        "save_directory = \"./deepseek-14B\"  # Kaydedilecek klasör\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c26QR9cX5rny"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 3. Model Yükleme\n",
        "# ----------------------------------------------\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Quantization ayarlarını yapılandır\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit kantizasyon kullan\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Kantizasyon tipi\n",
        "    bnb_4bit_use_double_quant=True,  # Çift kantizasyon kullan\n",
        ")\n",
        "\n",
        "\n",
        "# Modeli yükle\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,  # Quantization ayarlarını geçir\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))  # Yeni token için boyut ayarla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4emgVtu5t86"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 4. LoRA Konfigürasyonu #buradaki ayarları kendinize göre yapın, ben düşük seviyeli bir GPU ve iki saatlik Google Colab süresine göre ayarladım.\n",
        "# ----------------------------------------------\n",
        "peft_config = LoraConfig(\n",
        "    r=16,#64\n",
        "    lora_alpha=16,#64\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.001, #0.05\n",
        "    bias=\"none\",A\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGKKiFk25wda"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 5. Veri Hazırlama (Düzeltilmiş)\n",
        "# ----------------------------------------------\n",
        "def format_dataset(example):\n",
        "    return {\n",
        "        \"instruction\": example[\"instruction\"],\n",
        "        \"input\": example[\"input\"],\n",
        "        \"output\": example[\"output\"]\n",
        "    }\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/Veri-Kazima/colab/1alpaca_format.json\")[\"train\"]\n",
        "dataset = dataset.map(format_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrgMZzI252DO"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 6. Özelleştirilmiş Data Collator (DÜZELTİLMİŞ)\n",
        "# ----------------------------------------------\n",
        "def custom_collator(batch):\n",
        "    # Instruction + Input + Output formatını oluştur\n",
        "    texts = [\n",
        "        f\"### Instruction: {x['instruction']}\\n### Input: {x['input']}\\n### Output: {x['output']}\"\n",
        "        for x in batch\n",
        "    ]\n",
        "\n",
        "    # Tokenize işlemi (LABELS dahil)\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        max_length=64,#256 #buradaki ayarları kendinize göre yapın, ben düşük seviyeli bir GPU ve iki saatlik Google Colab süresine göre ayarladım.\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Labels = input_ids (Causal LM için)\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 7. Eğitim Ayarları #buradaki ayarları kendinize göre yapın, ben düşük seviyeli bir GPU ve iki saatlik Google Colab süresine göre ayarladım.\n",
        "# ----------------------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=  CHECKPOINT_DIR, #\"./output\", #çıkış yerine kendiniz seçin ama şimdi şu anlık değiştirmeyin\n",
        "    per_device_train_batch_size=16,  # 2 -> 1 (bellek tasarrufu)\n",
        "    gradient_accumulation_steps=1,   # 4 -> 8\n",
        "    num_train_epochs=1,              # Toplam 3 epoch\n",
        "    learning_rate=1e-4,              # 2e-4 -> 1e-4 (stabilite için)\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=200,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_8bit\"               # 8-bit optimizasyon\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMjc-LS454SU"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 8. Trainer (DÜZELTİLMİŞ)\n",
        "# ----------------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=custom_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9WdIYqU6KNQ"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# 9. Eğitimi Başlat\n",
        "# ----------------------------------------------\n",
        "try:\n",
        "    print(\"Checkpoint aranıyor...\")\n",
        "    trainer.train(resume_from_checkpoint=True)\n",
        "except Exception as e:\n",
        "    print(f\"Hata: {str(e)[:200]}...\\nYeni eğitim başlatılıyor...\")\n",
        "    trainer.train()\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 10. Modeli Kaydet\n",
        "# ----------------------------------------------\n",
        "model.save_pretrained(f\"{CHECKPOINT_DIR}/final_model\")\n",
        "tokenizer.save_pretrained(f\"{CHECKPOINT_DIR}/final_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osqcw_wL7Ddi"
      },
      "source": [
        "## PEFT ve LORA ile Ana Modeli Birleştirme\n",
        "Yukarıdaki model eğitiminde LORA ve PEFT ile ince ayar yaptık. Burada ise bize bazı dosyalar oluşturdu. Şimdi ise model eğitimi için kullandığımız ana model ile eğitim sonuçlarını birleştirmemiz gerekmektedir. Bu sayede ince ayar yaptığımız modeli rahatça kullanabileceğiz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-uXgmHS75Ub"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 1. Ana Model ve Adapter Yolları\n",
        "# ----------------------------------------------\n",
        "base_model_name = \"google/gemma-2b\"  # Orijinal model adı\n",
        "peft_model_path = \"/content/drive/MyDrive/Veri-Kazima/colab/output/checkpoint-7114\" #modeli kayıt ettiğiniz yer\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 2. Ana Modeli ve Tokenizer'ı Yükle\n",
        "# ----------------------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name,token=\"senin token\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 3. PEFT Adapter'ını Yükle\n",
        "# ----------------------------------------------\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    peft_model_path,\n",
        "    adapter_name=\"default\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ----------------------------------------------\n",
        "# 4. Modeli Birleştir ve Kullan\n",
        "# ----------------------------------------------\n",
        "model = model.merge_and_unload()  # Adapter'ı ana modele entegre et\n",
        "# Eğitim sonunda kaydetme\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Veri-Kazima/colab/birleştirilmiş_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Veri-Kazima/colab/birleştirilmiş_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGo9NKBT6mWM"
      },
      "source": [
        "## Model Eğitimini Kaydetme\n",
        "Model eğitimi bittiğinde tüm veriler cihazınızda kayıtlı olacaktır. Bu verileri Hugging Face içine yükleyip çalıştırabilirsiniz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJP_FWfD6pN4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"senin token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS4teaMr68d1"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Repo adı (örnek: kullanıcı_adı/model_adı)\n",
        "repo_name = \"denizzhansahin/gemma2bDeneme\"\n",
        "\n",
        "# Modeli yükle\n",
        "api = HfApi()\n",
        "#api.create_repo(repo_id=repo_name, repo_type=\"model\", private=True)  # Özel repo, oluşturmak için yorum satırını kullanma, güncellemek için yorum satırını kullan\n",
        "api.upload_folder(\n",
        "    folder_path=\"/content/drive/MyDrive/Veri-Kazima/colab/birleştirilmiş_model\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbOj5DtM8ZMK"
      },
      "source": [
        "## Model Çalıştırma\n",
        "Artık ince ayar yaptığımız ve gerekli düzenlemeleri yaptığımız modelimizi çalıştırabiliriz. Daha önce Hugging Face üzerine yüklediğimiz modeli çalıştıralım."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcX16jRS8qiZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"denizzhansahin/gemma2bDeneme\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"denizzhansahin/gemma2bDeneme\")\n",
        "\n",
        "input_text = \"Fenerbahçe\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
